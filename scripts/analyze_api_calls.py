#!/usr/bin/env python3
"""
APIå‘¼ã³å‡ºã—åˆ†æã‚¹ã‚¯ãƒªãƒ—ãƒˆã€‚LLMã¨Embeddingã®å‘¼ã³å‡ºã—å›æ•°ã¨å‡¦ç†æ™‚é–“ã‚’æŠ½å‡ºã™ã‚‹
"""

import re
import sys
from datetime import datetime
from collections import defaultdict

# å®šæ•°å®šç¾©
ERROR_PREFIX = "âŒ ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†å¤±æ•—:"


def parse_time(time_str):
    """æ™‚åˆ»æ–‡å­—åˆ—ã‚’datetimeã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã«å¤‰æ›"""
    try:
        # HH:MM:SSå½¢å¼
        time_obj = datetime.strptime(time_str, "%H:%M:%S")
        return time_obj
    except ValueError:
        return None


def analyze_log_file(log_file_path):
    """ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’åˆ†æã—ã¦APIå‘¼ã³å‡ºã—çµ±è¨ˆã‚’ç”Ÿæˆ"""

    llm_requests = []
    embedding_requests = []
    retry_events = []
    processing_summary = {}

    # ingestæ”¹å–„æ©Ÿèƒ½ã®åˆ†æãƒ‡ãƒ¼ã‚¿
    performance_data = []  # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°
    worker_optimization = {}  # ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°æœ€é©åŒ–
    file_size_warnings = []  # ãƒ¡ãƒ¢ãƒªåŠ¹ç‡æ”¹å–„
    chunk_analysis = []  # ãƒãƒ£ãƒ³ã‚­ãƒ³ã‚°æˆ¦ç•¥

    # ãƒªã‚¯ã‚¨ã‚¹ãƒˆé–‹å§‹æ™‚åˆ»ã‚’è¨˜éŒ²
    pending_llm = {}
    pending_embedding = {}

    try:
        with open(log_file_path, "r", encoding="utf-8") as f:
            for line_num, line in enumerate(f, 1):
                # æ™‚åˆ»ã‚’æŠ½å‡º
                time_match = re.match(r"^(\d{2}:\d{2}:\d{2})", line)

                # æ™‚åˆ»ã‚ã‚Šè¡Œã®å‡¦ç†
                if time_match:
                    time_str = time_match.group(1)
                    time_obj = parse_time(time_str)
                    if not time_obj:
                        continue

                    # ã‚¹ãƒ¬ãƒƒãƒ‰IDã¨ãƒ•ã‚¡ã‚¤ãƒ«åã‚’æŠ½å‡º [T123][filename]
                    thread_match = re.search(r"\[T(\d+)\]\[([^\]]+)\]", line)
                    thread_id = thread_match.group(1) if thread_match else "unknown"
                    file_name = thread_match.group(2) if thread_match else "unknown"
                else:
                    # æ™‚åˆ»ãªã—è¡Œã§ã‚‚å‡¦ç†ã‚µãƒãƒªãƒ¼ã¯è§£æ
                    time_obj = None
                    thread_id = "main"
                    file_name = "main"

                # LLM ãƒªã‚¯ã‚¨ã‚¹ãƒˆé–‹å§‹ï¼ˆæ™‚åˆ»ãŒã‚ã‚‹å ´åˆã®ã¿ï¼‰
                if (
                    time_obj
                    and "Sending HTTP Request: POST" in line
                    and "chat/completions" in line
                ):
                    key = f"{thread_id}_{file_name}"
                    pending_llm[key] = {
                        "start_time": time_obj,
                        "line_num": line_num,
                        "thread_id": thread_id,
                        "file_name": file_name,
                    }

                # LLM ãƒ¬ã‚¹ãƒãƒ³ã‚¹ï¼ˆæ™‚åˆ»ãŒã‚ã‚‹å ´åˆã®ã¿ï¼‰
                elif (
                    time_obj
                    and "HTTP Response: POST" in line
                    and "chat/completions" in line
                ):
                    key = f"{thread_id}_{file_name}"
                    if key in pending_llm:
                        start_info = pending_llm.pop(key)
                        duration = (time_obj - start_info["start_time"]).total_seconds()
                        llm_requests.append(
                            {
                                "thread_id": thread_id,
                                "file_name": file_name,
                                "start_time": start_info["start_time"],
                                "end_time": time_obj,
                                "duration": duration,
                                "start_line": start_info["line_num"],
                                "end_line": line_num,
                            }
                        )

                # Embedding ãƒªã‚¯ã‚¨ã‚¹ãƒˆé–‹å§‹ï¼ˆæ™‚åˆ»ãŒã‚ã‚‹å ´åˆã®ã¿ï¼‰
                elif (
                    time_obj
                    and "Sending HTTP Request: POST" in line
                    and "embeddings" in line
                ):
                    key = f"{thread_id}_{file_name}_{time_obj.timestamp()}"  # åŒæ™‚ä¸¦è¡Œã®ãŸã‚æ™‚åˆ»ã‚‚å«ã‚ã‚‹
                    pending_embedding[key] = {
                        "start_time": time_obj,
                        "line_num": line_num,
                        "thread_id": thread_id,
                        "file_name": file_name,
                    }

                # Embedding ãƒ¬ã‚¹ãƒãƒ³ã‚¹ï¼ˆæ™‚åˆ»ãŒã‚ã‚‹å ´åˆã®ã¿ï¼‰
                elif (
                    time_obj and "HTTP Response: POST" in line and "embeddings" in line
                ):
                    # æœ€ã‚‚è¿‘ã„é–‹å§‹æ™‚åˆ»ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆã¨ãƒãƒƒãƒãƒ³ã‚°
                    matching_key = None
                    min_diff = float("inf")

                    for key, start_info in pending_embedding.items():
                        if (
                            start_info["thread_id"] == thread_id
                            and start_info["file_name"] == file_name
                        ):
                            diff = abs(
                                (time_obj - start_info["start_time"]).total_seconds()
                            )
                            if diff < min_diff:
                                min_diff = diff
                                matching_key = key

                    if matching_key:
                        start_info = pending_embedding.pop(matching_key)
                        duration = (time_obj - start_info["start_time"]).total_seconds()
                        embedding_requests.append(
                            {
                                "thread_id": thread_id,
                                "file_name": file_name,
                                "start_time": start_info["start_time"],
                                "end_time": time_obj,
                                "duration": duration,
                                "start_line": start_info["line_num"],
                                "end_line": line_num,
                            }
                        )

                # Rate limitãƒªãƒˆãƒ©ã‚¤ã®æ¤œå‡ºï¼ˆæ™‚åˆ»ãŒã‚ã‚‹å ´åˆã®ã¿ï¼‰
                elif time_obj and "ğŸ”„ Rate limit detected" in line:
                    # ä¾‹: ğŸ”„ Rate limit detected. Waiting 61 seconds before retry (rate limit attempt 1/3)
                    wait_match = re.search(r"Waiting (\d+) seconds", line)
                    attempt_match = re.search(r"attempt (\d+)/(\d+)", line)

                    wait_time = int(wait_match.group(1)) if wait_match else 0
                    current_attempt = (
                        int(attempt_match.group(1)) if attempt_match else 0
                    )
                    max_attempts = int(attempt_match.group(2)) if attempt_match else 0

                    retry_events.append(
                        {
                            "type": "rate_limit",
                            "time": time_obj,
                            "file_name": file_name,
                            "wait_time": wait_time,
                            "attempt": current_attempt,
                            "max_attempts": max_attempts,
                            "line_num": line_num,
                        }
                    )

                # IndexErrorãƒªãƒˆãƒ©ã‚¤ã®æ¤œå‡ºï¼ˆæ™‚åˆ»ãŒã‚ã‚‹å ´åˆã®ã¿ï¼‰
                elif time_obj and "âš ï¸ Graphitiã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ç«¶åˆã‚¨ãƒ©ãƒ¼" in line:
                    # ä¾‹: âš ï¸ Graphitiã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ç«¶åˆã‚¨ãƒ©ãƒ¼ã€‚1ç§’å¾Œã«ãƒªãƒˆãƒ©ã‚¤ (index error attempt 1/3)
                    wait_match = re.search(r"(\d+)ç§’å¾Œã«ãƒªãƒˆãƒ©ã‚¤", line)
                    attempt_match = re.search(r"attempt (\d+)/(\d+)", line)

                    wait_time = int(wait_match.group(1)) if wait_match else 0
                    current_attempt = (
                        int(attempt_match.group(1)) if attempt_match else 0
                    )
                    max_attempts = int(attempt_match.group(2)) if attempt_match else 0

                    retry_events.append(
                        {
                            "type": "index_error",
                            "time": time_obj,
                            "file_name": file_name,
                            "wait_time": wait_time,
                            "attempt": current_attempt,
                            "max_attempts": max_attempts,
                            "line_num": line_num,
                        }
                    )

                # æœ€çµ‚å‡¦ç†çµæœã®æ¤œå‡º
                elif "ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆç™»éŒ²ãŒæ­£å¸¸ã«ç™»éŒ²ã•ã‚Œã¾ã—ãŸ" in line:
                    # å¾Œç¶šã®å‡¦ç†çµæœè¡Œã‚’æ¢ã™
                    pass
                elif "å‡¦ç†ãƒ•ã‚¡ã‚¤ãƒ«æ•°:" in line:
                    file_count_match = re.search(r"å‡¦ç†ãƒ•ã‚¡ã‚¤ãƒ«æ•°: (\d+)", line)
                    if file_count_match:
                        processing_summary["total_files"] = int(
                            file_count_match.group(1)
                        )
                elif "ä½œæˆãƒãƒ£ãƒ³ã‚¯æ•°:" in line:
                    chunk_count_match = re.search(r"ä½œæˆãƒãƒ£ãƒ³ã‚¯æ•°: (\d+)", line)
                    if chunk_count_match:
                        processing_summary["total_chunks"] = int(
                            chunk_count_match.group(1)
                        )
                elif "ç™»éŒ²ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰æ•°:" in line:
                    episode_count_match = re.search(r"ç™»éŒ²ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰æ•°: (\d+)", line)
                    if episode_count_match:
                        processing_summary["total_episodes"] = int(
                            episode_count_match.group(1)
                        )
                elif "âš ï¸ å‡¦ç†å¤±æ•—ãƒ•ã‚¡ã‚¤ãƒ«æ•°:" in line:
                    failed_count_match = re.search(r"å‡¦ç†å¤±æ•—ãƒ•ã‚¡ã‚¤ãƒ«æ•°: (\d+)", line)
                    if failed_count_match:
                        processing_summary["failed_files"] = int(
                            failed_count_match.group(1)
                        )

                # å¤±æ•—ãƒ•ã‚¡ã‚¤ãƒ«ã®è©³ç´°ã‚’åé›†ï¼ˆæ™‚åˆ»ãŒã‚ã‚‹å ´åˆã®ã¿ï¼‰
                elif time_obj and "âŒ ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†å¤±æ•—:" in line:
                    # ä¾‹: âŒ ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†å¤±æ•—: /data/input/SRv6-IaaS/ADR/images/tag_model.png - libGL.so.1: cannot open shared object file
                    file_match = re.search(r"âŒ ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†å¤±æ•—: ([^-]+) - (.+)", line)
                    if file_match:
                        file_path = file_match.group(1).strip()
                        error_message = file_match.group(2).strip()

                        if "failed_file_details" not in processing_summary:
                            processing_summary["failed_file_details"] = []

                        processing_summary["failed_file_details"].append(
                            {
                                "file_path": file_path,
                                "error_message": error_message,
                                "time": time_obj,
                                "line_num": line_num,
                            }
                        )

                # === ingestæ”¹å–„æ©Ÿèƒ½ã®åˆ†æ ===

                # 1. ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°æ©Ÿèƒ½
                elif "â±ï¸ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ -" in line:
                    # ä¾‹: â±ï¸ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ - rpc_callbacks.md (md): è§£æ 1.38ç§’, ãƒãƒ£ãƒ³ã‚¯åˆ†å‰² 0.01ç§’, ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ä½œæˆ 0.00ç§’, åˆè¨ˆ 1.39ç§’
                    perf_match = re.search(
                        r"â±ï¸ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ - (.+?) \((.+?)\): è§£æ ([\d.]+)ç§’, ãƒãƒ£ãƒ³ã‚¯åˆ†å‰² ([\d.]+)ç§’, ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ä½œæˆ ([\d.]+)ç§’, åˆè¨ˆ ([\d.]+)ç§’",
                        line,
                    )
                    if perf_match:
                        performance_data.append(
                            {
                                "file_name": perf_match.group(1),
                                "file_type": perf_match.group(2),
                                "parse_time": float(perf_match.group(3)),
                                "chunk_time": float(perf_match.group(4)),
                                "episode_time": float(perf_match.group(5)),
                                "total_time": float(perf_match.group(6)),
                                "time": time_obj,
                                "line_num": line_num,
                            }
                        )

                # 2. ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°æœ€é©åŒ–
                elif "ğŸ“Š ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°èª¿æ•´" in line:
                    # ä¾‹: ğŸ“Š ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°èª¿æ•´ - ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ç‡ 40.0%: 3 â†’ 4 ãƒ¯ãƒ¼ã‚«ãƒ¼
                    worker_match = re.search(
                        r"ğŸ“Š ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°èª¿æ•´ - (.+?): (\d+) â†’ (\d+) ãƒ¯ãƒ¼ã‚«ãƒ¼", line
                    )
                    if worker_match:
                        worker_optimization["adjustment_reason"] = worker_match.group(1)
                        worker_optimization["original_workers"] = int(
                            worker_match.group(2)
                        )
                        worker_optimization["optimized_workers"] = int(
                            worker_match.group(3)
                        )
                        worker_optimization["time"] = time_obj
                        worker_optimization["line_num"] = line_num

                elif "ğŸ“ˆ ãƒ•ã‚¡ã‚¤ãƒ«çµ±è¨ˆ" in line:
                    # ä¾‹: ğŸ“ˆ ãƒ•ã‚¡ã‚¤ãƒ«çµ±è¨ˆ - ç·æ•°: 5, ç”»åƒ: 2, PDF: 2, ãã®ä»–: 1
                    stats_match = re.search(
                        r"ğŸ“ˆ ãƒ•ã‚¡ã‚¤ãƒ«çµ±è¨ˆ - ç·æ•°: (\d+), ç”»åƒ: (\d+), PDF: (\d+), ãã®ä»–: (\d+)",
                        line,
                    )
                    if stats_match:
                        worker_optimization["file_stats"] = {
                            "total": int(stats_match.group(1)),
                            "images": int(stats_match.group(2)),
                            "pdfs": int(stats_match.group(3)),
                            "others": int(stats_match.group(4)),
                        }

                elif "ğŸš€ ä¸¦åˆ—å‡¦ç†ãƒ¢ãƒ¼ãƒ‰ã§å®Ÿè¡Œï¼ˆãƒ¯ãƒ¼ã‚«ãƒ¼æ•°:" in line:
                    # ä¾‹: ğŸš€ ä¸¦åˆ—å‡¦ç†ãƒ¢ãƒ¼ãƒ‰ã§å®Ÿè¡Œï¼ˆãƒ¯ãƒ¼ã‚«ãƒ¼æ•°: 3ï¼‰
                    parallel_match = re.search(
                        r"ğŸš€ ä¸¦åˆ—å‡¦ç†ãƒ¢ãƒ¼ãƒ‰ã§å®Ÿè¡Œï¼ˆãƒ¯ãƒ¼ã‚«ãƒ¼æ•°: (\d+)ï¼‰", line
                    )
                    if parallel_match:
                        worker_optimization["final_workers"] = int(
                            parallel_match.group(1)
                        )

                # 3. ãƒ¡ãƒ¢ãƒªåŠ¹ç‡æ”¹å–„
                elif "âš ï¸ å¤§ããªãƒ•ã‚¡ã‚¤ãƒ«æ¤œå‡º:" in line:
                    # ä¾‹: âš ï¸ å¤§ããªãƒ•ã‚¡ã‚¤ãƒ«æ¤œå‡º: Toodledoè¶…ã‚¿ã‚¹ã‚¯ç®¡ç†è¡“.pdf (57.4MB) - ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã«ã”æ³¨æ„ãã ã•ã„
                    size_match = re.search(
                        r"âš ï¸ å¤§ããªãƒ•ã‚¡ã‚¤ãƒ«æ¤œå‡º: (.+?) \(([\d.]+)MB\)", line
                    )
                    if size_match:
                        file_size_warnings.append(
                            {
                                "file_name": size_match.group(1),
                                "size_mb": float(size_match.group(2)),
                                "warning_type": "large_file",
                                "time": time_obj,
                                "line_num": line_num,
                            }
                        )

                elif "ğŸ“„ å¤§ãã‚ã®ãƒ•ã‚¡ã‚¤ãƒ«:" in line:
                    # ä¾‹: ğŸ“„ å¤§ãã‚ã®ãƒ•ã‚¡ã‚¤ãƒ«: document.pdf (75.2MB)
                    size_match = re.search(
                        r"ğŸ“„ å¤§ãã‚ã®ãƒ•ã‚¡ã‚¤ãƒ«: (.+?) \(([\d.]+)MB\)", line
                    )
                    if size_match:
                        file_size_warnings.append(
                            {
                                "file_name": size_match.group(1),
                                "size_mb": float(size_match.group(2)),
                                "warning_type": "medium_file",
                                "time": time_obj,
                                "line_num": line_num,
                            }
                        )

                # 4. ãƒãƒ£ãƒ³ã‚­ãƒ³ã‚°æˆ¦ç•¥ (ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ä½œæˆæ•°ã®æƒ…å ±ã‚‚åé›†)
                elif "ğŸ“¦ ä¸€æ‹¬ä¿å­˜é–‹å§‹ï¼ˆä¸¦åˆ—ï¼‰:" in line:
                    # ä¾‹: ğŸ“¦ ä¸€æ‹¬ä¿å­˜é–‹å§‹ï¼ˆä¸¦åˆ—ï¼‰: 661ä»¶ã®ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰
                    episode_match = re.search(
                        r"ğŸ“¦ ä¸€æ‹¬ä¿å­˜é–‹å§‹ï¼ˆä¸¦åˆ—ï¼‰: (\d+)ä»¶ã®ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰", line
                    )
                    if episode_match:
                        chunk_analysis.append(
                            {
                                "total_episodes": int(episode_match.group(1)),
                                "time": time_obj,
                                "line_num": line_num,
                            }
                        )

                elif "ğŸ“ ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†é–‹å§‹:" in line:
                    # ä¾‹: ğŸ“ ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†é–‹å§‹: rpc_callbacks.md (5ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰)
                    file_episode_match = re.search(
                        r"ğŸ“ ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†é–‹å§‹: (.+?) \((\d+)ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰\)", line
                    )
                    if file_episode_match:
                        # chunk_analysisã«å€‹åˆ¥ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰æ•°ã‚’è¿½åŠ 
                        if not chunk_analysis:
                            chunk_analysis.append({"file_episodes": []})
                        elif "file_episodes" not in chunk_analysis[-1]:
                            chunk_analysis[-1]["file_episodes"] = []

                        chunk_analysis[-1]["file_episodes"].append(
                            {
                                "file_name": file_episode_match.group(1),
                                "episode_count": int(file_episode_match.group(2)),
                            }
                        )

    except FileNotFoundError:
        print(f"ã‚¨ãƒ©ãƒ¼: ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {log_file_path}")
        return None
    except Exception as e:
        print(f"ã‚¨ãƒ©ãƒ¼: ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«è§£æä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {e}")
        return None

    return {
        "llm_requests": llm_requests,
        "embedding_requests": embedding_requests,
        "retry_events": retry_events,
        "processing_summary": processing_summary,
        "pending_llm": pending_llm,
        "pending_embedding": pending_embedding,
        # ingestæ”¹å–„æ©Ÿèƒ½ã®åˆ†æçµæœ
        "performance_data": performance_data,
        "worker_optimization": worker_optimization,
        "file_size_warnings": file_size_warnings,
        "chunk_analysis": chunk_analysis,
    }


def print_statistics(analysis_result):
    """çµ±è¨ˆæƒ…å ±ã‚’å‡ºåŠ›"""
    if not analysis_result:
        return

    llm_requests = analysis_result["llm_requests"]
    embedding_requests = analysis_result["embedding_requests"]

    print("=" * 80)
    print("APIå‘¼ã³å‡ºã—åˆ†æçµæœ")
    print("=" * 80)

    # LLMçµ±è¨ˆ
    print("\nğŸ¤– LLM APIå‘¼ã³å‡ºã— (chat/completions)")
    print(f"  ç·å‘¼ã³å‡ºã—å›æ•°: {len(llm_requests)}")

    if llm_requests:
        durations = [req["duration"] for req in llm_requests]
        avg_duration = sum(durations) / len(durations)
        max_duration = max(durations)
        min_duration = min(durations)
        total_duration = sum(durations)

        print(f"  å¹³å‡å‡¦ç†æ™‚é–“: {avg_duration:.2f}ç§’")
        print(f"  æœ€å¤§å‡¦ç†æ™‚é–“: {max_duration:.2f}ç§’")
        print(f"  æœ€å°å‡¦ç†æ™‚é–“: {min_duration:.2f}ç§’")
        print(f"  ç·å‡¦ç†æ™‚é–“: {total_duration:.2f}ç§’")

        # ãƒ•ã‚¡ã‚¤ãƒ«åˆ¥çµ±è¨ˆ
        file_stats = defaultdict(list)
        for req in llm_requests:
            file_stats[req["file_name"]].append(req["duration"])

        print("\n  ğŸ“ ãƒ•ã‚¡ã‚¤ãƒ«åˆ¥LLMå‘¼ã³å‡ºã—:")
        for file_name, durations in file_stats.items():
            count = len(durations)
            avg = sum(durations) / len(durations)
            total = sum(durations)
            print(f"    {file_name}: {count}å›, å¹³å‡{avg:.2f}ç§’, åˆè¨ˆ{total:.2f}ç§’")

    # Embeddingçµ±è¨ˆ
    print("\nğŸ”¤ Embedding APIå‘¼ã³å‡ºã— (/embeddings)")
    print(f"  ç·å‘¼ã³å‡ºã—å›æ•°: {len(embedding_requests)}")

    if embedding_requests:
        durations = [req["duration"] for req in embedding_requests]
        avg_duration = sum(durations) / len(durations)
        max_duration = max(durations)
        min_duration = min(durations)
        total_duration = sum(durations)

        print(f"  å¹³å‡å‡¦ç†æ™‚é–“: {avg_duration:.2f}ç§’")
        print(f"  æœ€å¤§å‡¦ç†æ™‚é–“: {max_duration:.2f}ç§’")
        print(f"  æœ€å°å‡¦ç†æ™‚é–“: {min_duration:.2f}ç§’")
        print(f"  ç·å‡¦ç†æ™‚é–“: {total_duration:.2f}ç§’")

        # ãƒ•ã‚¡ã‚¤ãƒ«åˆ¥çµ±è¨ˆ
        file_stats = defaultdict(list)
        for req in embedding_requests:
            file_stats[req["file_name"]].append(req["duration"])

        print("\n  ğŸ“ ãƒ•ã‚¡ã‚¤ãƒ«åˆ¥Embeddingå‘¼ã³å‡ºã—:")
        for file_name, durations in file_stats.items():
            count = len(durations)
            avg = sum(durations) / len(durations)
            total = sum(durations)
            print(f"    {file_name}: {count}å›, å¹³å‡{avg:.2f}ç§’, åˆè¨ˆ{total:.2f}ç§’")

    # æ¯”è¼ƒåˆ†æ
    print("\nğŸ“Š æ¯”è¼ƒåˆ†æ")
    if llm_requests and embedding_requests:
        llm_total = sum(req["duration"] for req in llm_requests)
        embedding_total = sum(req["duration"] for req in embedding_requests)
        grand_total = llm_total + embedding_total

        llm_percentage = (llm_total / grand_total) * 100
        embedding_percentage = (embedding_total / grand_total) * 100

        print(f"  LLMå‡¦ç†æ™‚é–“å‰²åˆ: {llm_percentage:.1f}% ({llm_total:.2f}ç§’)")
        print(
            f"  Embeddingå‡¦ç†æ™‚é–“å‰²åˆ: {embedding_percentage:.1f}% ({embedding_total:.2f}ç§’)"
        )
        print(f"  APIå‘¼ã³å‡ºã—ç·æ™‚é–“: {grand_total:.2f}ç§’")

    # æœªå®Œäº†ãƒªã‚¯ã‚¨ã‚¹ãƒˆè­¦å‘Š
    pending_llm = analysis_result["pending_llm"]
    pending_embedding = analysis_result["pending_embedding"]

    if pending_llm:
        print(f"\nâš ï¸  æœªå®Œäº†LLMãƒªã‚¯ã‚¨ã‚¹ãƒˆ: {len(pending_llm)}ä»¶")
        for key, info in pending_llm.items():
            print(f"    {info['file_name']} (line {info['line_num']})")

    if pending_embedding:
        print(f"\nâš ï¸  æœªå®Œäº†Embeddingãƒªã‚¯ã‚¨ã‚¹ãƒˆ: {len(pending_embedding)}ä»¶")
        for key, info in pending_embedding.items():
            print(f"    {info['file_name']} (line {info['line_num']})")

    # ãƒªãƒˆãƒ©ã‚¤åˆ†æ
    retry_events = analysis_result.get("retry_events", [])
    if retry_events:
        print("\nğŸ”„ ãƒªãƒˆãƒ©ã‚¤åˆ†æ")

        # Rate limitãƒªãƒˆãƒ©ã‚¤
        rate_limit_retries = [r for r in retry_events if r["type"] == "rate_limit"]
        if rate_limit_retries:
            print(f"  ğŸ“Š Rate Limitãƒªãƒˆãƒ©ã‚¤: {len(rate_limit_retries)}å›")

            wait_times = [r["wait_time"] for r in rate_limit_retries]
            if wait_times:
                avg_wait = sum(wait_times) / len(wait_times)
                total_wait = sum(wait_times)
                max_wait = max(wait_times)
                min_wait = min(wait_times)

                print(f"    å¹³å‡å¾…æ©Ÿæ™‚é–“: {avg_wait:.1f}ç§’")
                print(f"    æœ€å¤§å¾…æ©Ÿæ™‚é–“: {max_wait}ç§’")
                print(f"    æœ€å°å¾…æ©Ÿæ™‚é–“: {min_wait}ç§’")
                print(f"    ç·å¾…æ©Ÿæ™‚é–“: {total_wait}ç§’")

                # ãƒ•ã‚¡ã‚¤ãƒ«åˆ¥ãƒªãƒˆãƒ©ã‚¤çµ±è¨ˆ
                file_retries = defaultdict(list)
                for retry in rate_limit_retries:
                    file_retries[retry["file_name"]].append(retry["wait_time"])

                print("\n    ğŸ“ ãƒ•ã‚¡ã‚¤ãƒ«åˆ¥Rate Limitãƒªãƒˆãƒ©ã‚¤:")
                for file_name, wait_times in file_retries.items():
                    count = len(wait_times)
                    avg = sum(wait_times) / len(wait_times)
                    total = sum(wait_times)
                    print(
                        f"      {file_name}: {count}å›, å¹³å‡{avg:.1f}ç§’, åˆè¨ˆ{total}ç§’"
                    )

        # IndexErrorãƒªãƒˆãƒ©ã‚¤
        index_error_retries = [r for r in retry_events if r["type"] == "index_error"]
        if index_error_retries:
            print(f"\n  âš ï¸ IndexErrorãƒªãƒˆãƒ©ã‚¤: {len(index_error_retries)}å›")

            wait_times = [r["wait_time"] for r in index_error_retries]
            if wait_times:
                total_wait = sum(wait_times)
                print(f"    ç·å¾…æ©Ÿæ™‚é–“: {total_wait}ç§’")

                # ãƒ•ã‚¡ã‚¤ãƒ«åˆ¥çµ±è¨ˆ
                file_retries = defaultdict(int)
                for retry in index_error_retries:
                    file_retries[retry["file_name"]] += 1

                print("    ğŸ“ ãƒ•ã‚¡ã‚¤ãƒ«åˆ¥IndexErrorãƒªãƒˆãƒ©ã‚¤:")
                for file_name, count in file_retries.items():
                    print(f"      {file_name}: {count}å›")

    # å‡¦ç†ã‚µãƒãƒªãƒ¼
    processing_summary = analysis_result.get("processing_summary", {})
    if processing_summary:
        print("\nğŸ“‹ å‡¦ç†ã‚µãƒãƒªãƒ¼")

        if "total_files" in processing_summary:
            print(f"  ğŸ“ å‡¦ç†ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {processing_summary['total_files']}ä»¶")

        if "total_chunks" in processing_summary:
            print(f"  ğŸ”€ ä½œæˆãƒãƒ£ãƒ³ã‚¯æ•°: {processing_summary['total_chunks']}ä»¶")

        if "total_episodes" in processing_summary:
            print(f"  ğŸ“ ç™»éŒ²ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰æ•°: {processing_summary['total_episodes']}ä»¶")

        if "failed_files" in processing_summary:
            print(f"  âŒ å‡¦ç†å¤±æ•—ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {processing_summary['failed_files']}ä»¶")

            # å¤±æ•—ãƒ•ã‚¡ã‚¤ãƒ«ã®è©³ç´°è¡¨ç¤º
            if "failed_file_details" in processing_summary:
                print("\n    ğŸ“„ å¤±æ•—ãƒ•ã‚¡ã‚¤ãƒ«è©³ç´°:")
                for i, failure in enumerate(
                    processing_summary["failed_file_details"], 1
                ):
                    file_name = failure["file_path"].split("/")[
                        -1
                    ]  # ãƒ•ã‚¡ã‚¤ãƒ«åã®ã¿æŠ½å‡º
                    print(f"      {i}. {file_name}")
                    print(f"         ãƒ‘ã‚¹: {failure['file_path']}")
                    print(f"         ã‚¨ãƒ©ãƒ¼: {failure['error_message']}")

            # æˆåŠŸç‡è¨ˆç®—
            if "total_files" in processing_summary:
                total = processing_summary["total_files"]
                failed = processing_summary["failed_files"]
                success_rate = ((total - failed) / total) * 100 if total > 0 else 0
                print(f"  âœ… æˆåŠŸç‡: {success_rate:.1f}% ({total - failed}/{total})")

    # ç·åˆåˆ†æ
    if llm_requests and embedding_requests and retry_events:
        print("\nğŸ¯ ç·åˆåˆ†æ")

        # APIå‡¦ç†æ™‚é–“
        llm_total = sum(req["duration"] for req in llm_requests)
        embedding_total = sum(req["duration"] for req in embedding_requests)
        api_total = llm_total + embedding_total

        # ãƒªãƒˆãƒ©ã‚¤å¾…æ©Ÿæ™‚é–“
        retry_total = sum(r["wait_time"] for r in retry_events)

        # å…¨ä½“æ™‚é–“ã«å¯¾ã™ã‚‹æ¯”ç‡
        if api_total > 0:
            retry_ratio = (retry_total / api_total) * 100
            print(f"  â±ï¸ APIå‡¦ç†æ™‚é–“: {api_total:.0f}ç§’")
            print(f"  â³ ãƒªãƒˆãƒ©ã‚¤å¾…æ©Ÿæ™‚é–“: {retry_total}ç§’")
            print(f"  ğŸ“ˆ ãƒªãƒˆãƒ©ã‚¤æ™‚é–“æ¯”ç‡: {retry_ratio:.1f}%")

        # ãƒªãƒˆãƒ©ã‚¤åŠ¹æœ
        rate_limit_count = len([r for r in retry_events if r["type"] == "rate_limit"])
        if rate_limit_count > 0:
            print(f"  ğŸ›¡ï¸ Rate Limitãƒªãƒˆãƒ©ã‚¤ã«ã‚ˆã‚‹å›å¾©: {rate_limit_count}å›")


def print_ingest_improvements_analysis(analysis_result):
    """ingestæ©Ÿèƒ½ã®åˆ†æçµæœã‚’å‡ºåŠ›"""
    if not analysis_result:
        return

    performance_data = analysis_result.get("performance_data", [])
    worker_optimization = analysis_result.get("worker_optimization", {})
    file_size_warnings = analysis_result.get("file_size_warnings", [])
    chunk_analysis = analysis_result.get("chunk_analysis", [])

    print("\n" + "=" * 80)
    print("ğŸ“ˆ INGESTå‹•ä½œåˆ†æ")
    print("=" * 80)

    # 1. ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°æ©Ÿèƒ½
    if performance_data:
        print("\nğŸ¯ 1. ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°æ©Ÿèƒ½ âœ…")
        print(f"  ç›£è¦–å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(performance_data)}ä»¶")

        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚¿ã‚¤ãƒ—åˆ¥ã®çµ±è¨ˆ
        by_type = {}
        for data in performance_data:
            file_type = data["file_type"]
            if file_type not in by_type:
                by_type[file_type] = []
            by_type[file_type].append(data)

        print(f"  ãƒ•ã‚¡ã‚¤ãƒ«ã‚¿ã‚¤ãƒ—: {', '.join(by_type.keys())}")

        # å„å‡¦ç†ã‚¹ãƒ†ãƒƒãƒ—ã®çµ±è¨ˆ
        total_parse = sum(data["parse_time"] for data in performance_data)
        total_chunk = sum(data["chunk_time"] for data in performance_data)
        total_episode = sum(data["episode_time"] for data in performance_data)
        total_processing = sum(data["total_time"] for data in performance_data)

        print("\n  ğŸ“Š å‡¦ç†æ™‚é–“çµ±è¨ˆ:")
        print(
            f"    è§£æå‡¦ç†: {total_parse:.2f}ç§’ ({total_parse / total_processing * 100:.1f}%)"
        )
        print(
            f"    ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²: {total_chunk:.2f}ç§’ ({total_chunk / total_processing * 100:.1f}%)"
        )
        print(
            f"    ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ä½œæˆ: {total_episode:.2f}ç§’ ({total_episode / total_processing * 100:.1f}%)"
        )
        print(f"    ç·å‡¦ç†æ™‚é–“: {total_processing:.2f}ç§’")

        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚¿ã‚¤ãƒ—åˆ¥è©³ç´°
        print("\n  ğŸ“ ãƒ•ã‚¡ã‚¤ãƒ«ã‚¿ã‚¤ãƒ—åˆ¥ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹:")
        for file_type, data_list in by_type.items():
            avg_parse = sum(d["parse_time"] for d in data_list) / len(data_list)
            avg_total = sum(d["total_time"] for d in data_list) / len(data_list)
            print(
                f"    {file_type}: å¹³å‡è§£æ{avg_parse:.2f}ç§’, å¹³å‡åˆè¨ˆ{avg_total:.2f}ç§’ ({len(data_list)}ãƒ•ã‚¡ã‚¤ãƒ«)"
            )

        # æœ€ã‚‚æ™‚é–“ã®ã‹ã‹ã£ãŸãƒ•ã‚¡ã‚¤ãƒ«
        slowest = max(performance_data, key=lambda x: x["total_time"])
        fastest = min(performance_data, key=lambda x: x["total_time"])
        print("\n  â±ï¸ å‡¦ç†æ™‚é–“:")
        print(
            f"    æœ€é•·: {slowest['file_name']} ({slowest['file_type']}) - {slowest['total_time']:.2f}ç§’"
        )
        print(
            f"    æœ€çŸ­: {fastest['file_name']} ({fastest['file_type']}) - {fastest['total_time']:.2f}ç§’"
        )

    # 2. ä¸¦åˆ—å‡¦ç†æœ€é©åŒ–
    if worker_optimization:
        print("\nâš¡ 2. ä¸¦åˆ—å‡¦ç†æœ€é©åŒ– âœ…")

        if "file_stats" in worker_optimization:
            stats = worker_optimization["file_stats"]
            print(
                f"  ãƒ•ã‚¡ã‚¤ãƒ«çµ±è¨ˆ: ç·æ•°{stats['total']}, ç”»åƒ{stats['images']}, PDF{stats['pdfs']}, ãã®ä»–{stats['others']}"
            )

            # æ¯”ç‡è¨ˆç®—
            if stats["total"] > 0:
                image_ratio = stats["images"] / stats["total"] * 100
                pdf_ratio = stats["pdfs"] / stats["total"] * 100
                print(f"  ãƒ•ã‚¡ã‚¤ãƒ«æ¯”ç‡: ç”»åƒ{image_ratio:.1f}%, PDF{pdf_ratio:.1f}%")

        if "adjustment_reason" in worker_optimization:
            print(f"  ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°èª¿æ•´: {worker_optimization['adjustment_reason']}")
            print(
                f"  {worker_optimization['original_workers']} â†’ {worker_optimization['optimized_workers']} ãƒ¯ãƒ¼ã‚«ãƒ¼"
            )

        if "final_workers" in worker_optimization:
            print(f"  æœ€çµ‚ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°: {worker_optimization['final_workers']}")

        # æœ€é©åŒ–åŠ¹æœã®è©•ä¾¡
        if (
            "original_workers" in worker_optimization
            and "optimized_workers" in worker_optimization
        ):
            original = worker_optimization["original_workers"]
            optimized = worker_optimization["optimized_workers"]
            if optimized != original:
                print(
                    f"  ğŸ¯ æœ€é©åŒ–åŠ¹æœ: ãƒ•ã‚¡ã‚¤ãƒ«ç‰¹æ€§ã«å¿œã˜ã¦{abs(optimized - original)}ãƒ¯ãƒ¼ã‚«ãƒ¼{'å¢—åŠ ' if optimized > original else 'å‰Šæ¸›'}"
                )
            else:
                print("  ğŸ¯ æœ€é©åŒ–åŠ¹æœ: ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šãŒæœ€é©ã¨åˆ¤å®š")

    # 3. ãƒ¡ãƒ¢ãƒªåŠ¹ç‡æ”¹å–„
    if file_size_warnings:
        print("\nğŸ’¾ 3. ãƒ¡ãƒ¢ãƒªåŠ¹ç‡æ”¹å–„ âœ…")
        print(f"  ç›£è¦–å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(file_size_warnings)}ä»¶")

        large_files = [
            w for w in file_size_warnings if w["warning_type"] == "large_file"
        ]
        medium_files = [
            w for w in file_size_warnings if w["warning_type"] == "medium_file"
        ]

        if large_files:
            print(f"  âš ï¸ å¤§ããªãƒ•ã‚¡ã‚¤ãƒ«(100MB+): {len(large_files)}ä»¶")
            for warning in large_files:
                print(f"    {warning['file_name']}: {warning['size_mb']:.1f}MB")

        if medium_files:
            print(f"  ğŸ“„ å¤§ãã‚ã®ãƒ•ã‚¡ã‚¤ãƒ«(50-100MB): {len(medium_files)}ä»¶")
            for warning in medium_files:
                print(f"    {warning['file_name']}: {warning['size_mb']:.1f}MB")

        if file_size_warnings:
            total_size = sum(w["size_mb"] for w in file_size_warnings)
            avg_size = total_size / len(file_size_warnings)
            max_size = max(w["size_mb"] for w in file_size_warnings)
            print(
                f"  ğŸ“Š ã‚µã‚¤ã‚ºçµ±è¨ˆ: å¹³å‡{avg_size:.1f}MB, æœ€å¤§{max_size:.1f}MB, åˆè¨ˆ{total_size:.1f}MB"
            )

    # 4. ãƒãƒ£ãƒ³ã‚­ãƒ³ã‚°æˆ¦ç•¥æ”¹å–„
    if chunk_analysis:
        print("\nğŸ”€ 4. ãƒãƒ£ãƒ³ã‚­ãƒ³ã‚°æˆ¦ç•¥æ”¹å–„ âœ…")

        for analysis in chunk_analysis:
            if "total_episodes" in analysis:
                print(f"  ç·ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰æ•°: {analysis['total_episodes']}ä»¶")

            if "file_episodes" in analysis:
                print("  ãƒ•ã‚¡ã‚¤ãƒ«åˆ¥ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰æ•°:")
                file_episodes = analysis["file_episodes"]
                total_files = len(file_episodes)
                total_episodes = sum(fe["episode_count"] for fe in file_episodes)

                # ãƒ•ã‚¡ã‚¤ãƒ«åˆ¥è©³ç´°
                for file_ep in file_episodes:
                    ratio = (
                        file_ep["episode_count"] / total_episodes * 100
                        if total_episodes > 0
                        else 0
                    )
                    print(
                        f"    {file_ep['file_name']}: {file_ep['episode_count']}ä»¶ ({ratio:.1f}%)"
                    )

                # çµ±è¨ˆæƒ…å ±
                if file_episodes:
                    avg_episodes = total_episodes / total_files
                    max_episodes = max(fe["episode_count"] for fe in file_episodes)
                    min_episodes = min(fe["episode_count"] for fe in file_episodes)
                    print(
                        f"  ğŸ“Š ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰çµ±è¨ˆ: å¹³å‡{avg_episodes:.1f}ä»¶/ãƒ•ã‚¡ã‚¤ãƒ«, æœ€å¤§{max_episodes}ä»¶, æœ€å°{min_episodes}ä»¶"
                    )


def print_failed_files(log_file_path):
    """å¤±æ•—ãƒ•ã‚¡ã‚¤ãƒ«ã®è©³ç´°ã‚’è¡¨ç¤º"""
    try:
        failed_files = []
        with open(log_file_path, "r", encoding="utf-8") as f:
            for line in f:
                if ERROR_PREFIX in line:
                    failed_files.append(line.strip())

        if failed_files:
            print(f"\nğŸ“„ å¤±æ•—ãƒ•ã‚¡ã‚¤ãƒ«è©³ç´°: {len(failed_files)}ä»¶")
            for i, line in enumerate(failed_files, 1):
                # ãƒ­ã‚°è¡Œã‹ã‚‰æƒ…å ±ã‚’æŠ½å‡º
                # ä¾‹: 02:42:28 [T184][tag_model] - src.usecase.register_document_usecase - ERROR - âŒ ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†å¤±æ•—: /data/input/.../file.png - error message
                if ERROR_PREFIX in line:
                    # ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã¨ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’æŠ½å‡º
                    parts = line.split(ERROR_PREFIX)
                    if len(parts) > 1:
                        file_and_error = parts[1].strip()
                        if " - " in file_and_error:
                            file_path, error_msg = file_and_error.split(" - ", 1)
                            file_name = file_path.strip().split("/")[
                                -1
                            ]  # ãƒ•ã‚¡ã‚¤ãƒ«åã®ã¿
                            print(f"  {i}. {file_name}")
                            print(f"     ãƒ‘ã‚¹: {file_path.strip()}")
                            print(f"     ã‚¨ãƒ©ãƒ¼: {error_msg.strip()}")
    except Exception as e:
        print(f"å¤±æ•—ãƒ•ã‚¡ã‚¤ãƒ«è©³ç´°ã®å–å¾—ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")


def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    if len(sys.argv) != 2:
        print("ä½¿ç”¨æ–¹æ³•: python analyze_api_calls.py <ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹>")
        print("ä¾‹: python tmp/analyze_api_calls.py tmp/ingest-example-parallel.log")
        sys.exit(1)

    log_file_path = sys.argv[1]

    print(f"ğŸ“Š ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«åˆ†æä¸­: {log_file_path}")

    analysis_result = analyze_log_file(log_file_path)
    print_statistics(analysis_result)

    # ingestæ”¹å–„æ©Ÿèƒ½ã®åˆ†æçµæœã‚’è¡¨ç¤º
    print_ingest_improvements_analysis(analysis_result)

    # å¤±æ•—ãƒ•ã‚¡ã‚¤ãƒ«ã®è©³ç´°ã‚’è¡¨ç¤º
    print_failed_files(log_file_path)


if __name__ == "__main__":
    main()
