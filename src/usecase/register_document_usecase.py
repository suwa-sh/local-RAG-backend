"""RegisterDocumentUseCase - ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆç™»éŒ²ã®ãƒ¦ãƒ¼ã‚¹ã‚±ãƒ¼ã‚¹"""

import logging
import multiprocessing
import time
from collections import Counter
from dataclasses import dataclass
from pathlib import Path
from typing import List, Tuple, Any, Dict
from concurrent.futures import ThreadPoolExecutor, as_completed

from src.domain.group_id import GroupId
from src.adapter.filesystem_document_reader import FileSystemDocumentReader
from src.adapter.unstructured_document_parser import UnstructuredDocumentParser
from src.adapter.graphiti_episode_repository import GraphitiEpisodeRepository
from src.adapter.chunk_file_manager import ChunkFileManager
from src.main.settings import load_config


@dataclass
class RegisterResult:
    """ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆç™»éŒ²çµæœ"""

    total_files: int
    total_chunks: int
    total_episodes: int
    success: bool
    error_message: str = ""


class RegisterDocumentUseCase:
    """ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆç™»éŒ²ã®ãƒ¦ãƒ¼ã‚¹ã‚±ãƒ¼ã‚¹"""

    def __init__(
        self,
        file_reader: FileSystemDocumentReader,
        document_parser: UnstructuredDocumentParser,
        episode_repository: GraphitiEpisodeRepository,
        chunk_file_manager: ChunkFileManager | None = None,
    ) -> None:
        """
        RegisterDocumentUseCaseã‚’åˆæœŸåŒ–ã™ã‚‹

        Args:
            file_reader: ãƒ•ã‚¡ã‚¤ãƒ«ã‚·ã‚¹ãƒ†ãƒ ã‹ã‚‰ã®èª­ã¿è¾¼ã¿
            document_parser: ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆè§£æã¨ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²
            episode_repository: ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ä¿å­˜
            chunk_file_manager: ãƒãƒ£ãƒ³ã‚¯ãƒ•ã‚¡ã‚¤ãƒ«ç®¡ç†ï¼ˆNoneã®å ´åˆã¯è‡ªå‹•ç”Ÿæˆï¼‰
        """
        self._file_reader = file_reader
        self._document_parser = document_parser
        self._episode_repository = episode_repository
        self._chunk_file_manager = chunk_file_manager or ChunkFileManager()
        self._logger = logging.getLogger(__name__)

    def _determine_optimal_workers(
        self, documents: List, requested_workers: int
    ) -> int:
        """
        ãƒ•ã‚¡ã‚¤ãƒ«ã‚¿ã‚¤ãƒ—ã«å¿œã˜ã¦æœ€é©ãªãƒ¯ãƒ¼ã‚«ãƒ¼æ•°ã‚’æ±ºå®šã™ã‚‹

        Args:
            documents: å‡¦ç†å¯¾è±¡ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãƒªã‚¹ãƒˆ
            requested_workers: ãƒªã‚¯ã‚¨ã‚¹ãƒˆã•ã‚ŒãŸãƒ¯ãƒ¼ã‚«ãƒ¼æ•°

        Returns:
            int: æœ€é©åŒ–ã•ã‚ŒãŸãƒ¯ãƒ¼ã‚«ãƒ¼æ•°
        """
        if not documents:
            return 1

        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚¿ã‚¤ãƒ—ã®çµ±è¨ˆã‚’å–å¾—
        file_types = [doc.file_type for doc in documents]
        type_counter = Counter(file_types)
        total_files = len(documents)

        # ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¿ã‚¤ãƒ—ï¼ˆCPUè² è·ãŒé«˜ã„ï¼‰
        image_types = {"png", "jpg", "jpeg", "bmp", "tiff", "tif", "heic"}
        image_count = sum(
            count
            for file_type, count in type_counter.items()
            if file_type.lower() in image_types
        )

        # PDF ãƒ•ã‚¡ã‚¤ãƒ«ã®æ•°ï¼ˆãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãŒå¤šã„ï¼‰
        pdf_count = type_counter.get("pdf", 0)

        # ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã®å‰²åˆ
        image_ratio = image_count / total_files
        pdf_ratio = pdf_count / total_files

        cpu_count = multiprocessing.cpu_count()

        # æœ€é©ãªãƒ¯ãƒ¼ã‚«ãƒ¼æ•°ã‚’æ±ºå®š
        if image_ratio > 0.5:  # ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ãŒ50%ä»¥ä¸Š
            optimal_workers = min(cpu_count // 2, total_files, 4)
            self._logger.info(
                f"ğŸ“Š ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°èª¿æ•´ - ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ç‡ {image_ratio:.1%}: "
                f"{requested_workers} â†’ {optimal_workers} ãƒ¯ãƒ¼ã‚«ãƒ¼"
            )
        elif pdf_ratio > 0.7:  # PDFãƒ•ã‚¡ã‚¤ãƒ«ãŒ70%ä»¥ä¸Š
            optimal_workers = min(cpu_count // 2, total_files, 6)
            self._logger.info(
                f"ğŸ“Š ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°èª¿æ•´ - PDFç‡ {pdf_ratio:.1%}: "
                f"{requested_workers} â†’ {optimal_workers} ãƒ¯ãƒ¼ã‚«ãƒ¼"
            )
        else:  # ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ãŒå¤šã„å ´åˆã¯ç©æ¥µçš„ã«ä¸¦åˆ—åŒ–
            optimal_workers = min(cpu_count, total_files, requested_workers)
            if optimal_workers != requested_workers:
                self._logger.info(
                    f"ğŸ“Š ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°èª¿æ•´ - è»½é‡ãƒ•ã‚¡ã‚¤ãƒ«ä¸­å¿ƒ: "
                    f"{requested_workers} â†’ {optimal_workers} ãƒ¯ãƒ¼ã‚«ãƒ¼"
                )

        # ãƒ•ã‚¡ã‚¤ãƒ«ç¨®åˆ¥ã®çµ±è¨ˆã‚’ãƒ­ã‚°å‡ºåŠ›
        self._logger.info(
            f"ğŸ“ˆ ãƒ•ã‚¡ã‚¤ãƒ«çµ±è¨ˆ - ç·æ•°: {total_files}, ç”»åƒ: {image_count}, "
            f"PDF: {pdf_count}, ãã®ä»–: {total_files - image_count - pdf_count}"
        )

        return max(1, optimal_workers)  # æœ€ä½1ãƒ¯ãƒ¼ã‚«ãƒ¼ã¯ç¢ºä¿

    def _process_single_document_with_recovery(
        self,
        document,
        group_id: GroupId,
        index: int,
        total: int,
        base_directory: str = None,
    ) -> Tuple[List, int, str]:
        """
        å˜ä¸€ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®çµ±ä¸€å‡¦ç†ãƒ•ãƒ­ãƒ¼ï¼ˆã‚¨ãƒ©ãƒ¼å†å‡¦ç†å¯¾å¿œï¼‰

        Args:
            document: ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ
            group_id: ã‚°ãƒ«ãƒ¼ãƒ—ID
            index: ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
            total: ç·ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°
            base_directory: åŸºæº–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªï¼ˆãƒ•ã‚¡ã‚¤ãƒ«ç§»å‹•ç”¨ï¼‰

        Returns:
            Tuple[List, int, str]: (ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ãƒªã‚¹ãƒˆ, ãƒãƒ£ãƒ³ã‚¯æ•°, ã‚¨ãƒ©ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹)
        """
        from src.adapter.logging_utils import current_file

        # ç¾åœ¨å‡¦ç†ä¸­ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«è¨­å®š
        current_file.set(document.file_path)

        try:
            self._logger.info(
                f"ğŸ”„ ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ä¸­ ({index}/{total}): {document.file_name}"
            )

            # å…¨ä½“ã®å‡¦ç†æ™‚é–“è¨ˆæ¸¬é–‹å§‹
            start_time = time.time()

            # ã‚¹ãƒ†ãƒƒãƒ—1: ãƒãƒ£ãƒ³ã‚¯ãƒ•ã‚¡ã‚¤ãƒ«ã®å­˜åœ¨ç¢ºèª
            chunks = []
            if self._chunk_file_manager.has_chunk_files(document.file_path):
                # ä¿å­˜ã•ã‚ŒãŸãƒãƒ£ãƒ³ã‚¯ã‚’èª­ã¿è¾¼ã¿
                self._logger.info(f"ğŸ“ ä¿å­˜æ¸ˆã¿ãƒãƒ£ãƒ³ã‚¯èª­ã¿è¾¼ã¿: {document.file_name}")
                chunks, metadata = self._chunk_file_manager.load_chunks(
                    document.file_path
                )

                # æœ€å¾Œã«å‡¦ç†ã•ã‚ŒãŸä½ç½®ã‚’å–å¾—
                last_processed = metadata.get("last_processed_position", -1)

                # æœªå‡¦ç†ã®ãƒãƒ£ãƒ³ã‚¯ã®ã¿ã‚’å¯¾è±¡ã¨ã™ã‚‹
                if last_processed >= 0:
                    chunks = chunks[last_processed + 1 :]
                    self._logger.info(
                        f"ğŸ”„ å†å‡¦ç†é–‹å§‹: ä½ç½® {last_processed + 1} ã‹ã‚‰ {len(chunks)}ãƒãƒ£ãƒ³ã‚¯"
                    )

            else:
                # æ–°è¦å‡¦ç†: å…ƒãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒãƒ£ãƒ³ã‚¯ç”Ÿæˆ
                self._logger.debug(f"ğŸ†• æ–°è¦å‡¦ç†: {document.file_name}")

                # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’è§£æ
                parse_start = time.time()
                elements = self._document_parser.parse(document.file_path)
                parse_time = time.time() - parse_start
                self._logger.debug(f"ğŸ“ è§£æå®Œäº† - è¦ç´ æ•°: {len(elements)}")

                # ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²
                chunk_start = time.time()
                chunks = self._document_parser.split_elements(elements, document)
                chunk_time = time.time() - chunk_start
                self._logger.debug(f"ğŸ”€ ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²å®Œäº† - ãƒãƒ£ãƒ³ã‚¯æ•°: {len(chunks)}")

                # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æƒ…å ±ã‚’ãƒ­ã‚°å‡ºåŠ›
                self._logger.info(
                    f"â±ï¸ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ - {document.file_name} ({document.file_type}): "
                    f"è§£æ {parse_time:.2f}ç§’, ãƒãƒ£ãƒ³ã‚¯åˆ†å‰² {chunk_time:.2f}ç§’"
                )

            # ã‚¹ãƒ†ãƒƒãƒ—2: ãƒãƒ£ãƒ³ã‚¯ã®é †æ¬¡ç™»éŒ²
            episodes = []
            total_chunks = len(chunks)

            if total_chunks == 0:
                self._logger.warning(f"âš ï¸ å‡¦ç†å¯¾è±¡ãƒãƒ£ãƒ³ã‚¯ãªã—: {document.file_name}")
                # ãƒãƒ£ãƒ³ã‚¯ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã™ã‚‹å ´åˆã¯å‰Šé™¤
                if self._chunk_file_manager.has_chunk_files(document.file_path):
                    self._chunk_file_manager.delete_all_chunks(document.file_path)
                return [], 0, None

            for i, chunk in enumerate(chunks):
                try:
                    # ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚’ä½œæˆ
                    episode = chunk.to_episode(group_id)
                    episodes.append(episode)

                    self._logger.debug(
                        f"ğŸ“‹ ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ä½œæˆå®Œäº† ({i + 1}/{total_chunks}): {episode.name}"
                    )

                except Exception as e:
                    # ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿæ™‚: æ®‹ã‚Šã®ãƒãƒ£ãƒ³ã‚¯ã‚’ä¿å­˜
                    remaining_chunks = chunks[i:]
                    last_processed_position = i - 1 if i > 0 else -1

                    self._logger.error(
                        f"âŒ ãƒãƒ£ãƒ³ã‚¯å‡¦ç†ã‚¨ãƒ©ãƒ¼ (ä½ç½®: {i}): {document.file_name} - {e}"
                    )

                    # æ®‹ã‚Šãƒãƒ£ãƒ³ã‚¯ã‚’ä¿å­˜
                    self._chunk_file_manager.save_chunks(
                        remaining_chunks,
                        document.file_path,
                        last_processed_position,
                        str(e),
                    )

                    return episodes, len(episodes), document.file_path

            # ã‚¹ãƒ†ãƒƒãƒ—3: å…¨æˆåŠŸæ™‚ã®å‡¦ç†
            total_time = time.time() - start_time
            self._logger.info(
                f"âœ… ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†å®Œäº†: {document.file_name} "
                f"({total_chunks}ãƒãƒ£ãƒ³ã‚¯, {total_time:.2f}ç§’)"
            )

            # ãƒãƒ£ãƒ³ã‚¯ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã™ã‚‹å ´åˆã¯å‰Šé™¤ï¼ˆå‡¦ç†å®Œäº†ã®ãŸã‚ï¼‰
            if self._chunk_file_manager.has_chunk_files(document.file_path):
                self._chunk_file_manager.delete_all_chunks(document.file_path)

            # ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ãŒç”Ÿæˆã•ã‚Œã¦ã„ã‚‹å ´åˆã€ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜ã—ã¦work/ã«ç§»å‹•
            if episodes and base_directory and "/input" in document.file_path:
                try:
                    # ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
                    self._chunk_file_manager.save_episodes(document.file_path, episodes)

                    # ãƒ•ã‚¡ã‚¤ãƒ«ç§»å‹• (input â†’ work)
                    work_directory = base_directory.replace("/input", "/work")
                    new_path = self._file_reader.move_file(
                        document.file_path, work_directory
                    )

                    # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®ãƒ‘ã‚¹ã‚’æ›´æ–°
                    document.file_path = new_path

                    self._logger.info(
                        f"ğŸ“ å‡¦ç†ä¸­ãƒ•ã‚¡ã‚¤ãƒ«ç§»å‹•: {document.file_name} â†’ work/"
                    )
                except Exception as e:
                    self._logger.error(
                        f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«ç§»å‹•å¤±æ•—: {document.file_name} - {e}"
                    )
                    # ç§»å‹•ã«å¤±æ•—ã—ã¦ã‚‚ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã¯è¿”ã™

            return episodes, total_chunks, None

        except Exception as e:
            error_msg = f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†å¤±æ•—: {document.file_path} - {e}"
            self._logger.error(error_msg)
            return [], 0, document.file_path

    def _process_single_document(
        self, document, group_id: GroupId, index: int, total: int
    ) -> Tuple[List, int, str]:
        """å˜ä¸€ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®å‡¦ç†ï¼ˆä¸¦åˆ—å®Ÿè¡Œç”¨ï¼‰"""
        from src.adapter.logging_utils import current_file

        # ç¾åœ¨å‡¦ç†ä¸­ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«è¨­å®š
        current_file.set(document.file_path)

        try:
            self._logger.info(
                f"ğŸ”„ ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ä¸­ ({index}/{total}): {document.file_name}"
            )

            # å…¨ä½“ã®å‡¦ç†æ™‚é–“è¨ˆæ¸¬é–‹å§‹
            start_time = time.time()

            # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’è§£æ
            parse_start = time.time()
            elements = self._document_parser.parse(document.file_path)
            parse_time = time.time() - parse_start
            self._logger.debug(f"ğŸ“ è§£æå®Œäº† - è¦ç´ æ•°: {len(elements)}")

            # ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²
            chunk_start = time.time()
            chunks = self._document_parser.split_elements(elements, document)
            chunk_time = time.time() - chunk_start
            self._logger.debug(f"ğŸ”€ ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²å®Œäº† - ãƒãƒ£ãƒ³ã‚¯æ•°: {len(chunks)}")

            # ãƒãƒ£ãƒ³ã‚¯ã‹ã‚‰ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚’ä½œæˆ
            episode_start = time.time()
            episodes = []
            for j, chunk in enumerate(chunks):
                episode = chunk.to_episode(group_id)
                episodes.append(episode)
                self._logger.debug(
                    f"ğŸ“‹ ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ä½œæˆ ({j + 1}/{len(chunks)}): {episode.name}"
                )
            episode_time = time.time() - episode_start

            # å…¨ä½“ã®å‡¦ç†æ™‚é–“
            total_time = time.time() - start_time

            # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æƒ…å ±ã‚’ãƒ­ã‚°å‡ºåŠ›
            self._logger.info(
                f"â±ï¸ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ - {document.file_name} ({document.file_type}): "
                f"è§£æ {parse_time:.2f}ç§’, ãƒãƒ£ãƒ³ã‚¯åˆ†å‰² {chunk_time:.2f}ç§’, "
                f"ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ä½œæˆ {episode_time:.2f}ç§’, åˆè¨ˆ {total_time:.2f}ç§’"
            )

            return episodes, len(chunks), None

        except Exception as e:
            error_msg = f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†å¤±æ•—: {document.file_path} - {e}"
            self._logger.error(error_msg)
            return [], 0, document.file_path

    async def execute_parallel(
        self,
        group_id: GroupId,
        directory: str,
        max_workers: int = 3,
        register_workers: int = 2,
    ) -> RegisterResult:
        """
        ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆç™»éŒ²ã®å®Ÿè¡Œï¼ˆ2æ®µéšä¸¦åˆ—å‡¦ç†ç‰ˆï¼‰

        Args:
            group_id: ã‚°ãƒ«ãƒ¼ãƒ—ID
            directory: å¯¾è±¡ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
            max_workers: ãƒãƒ£ãƒ³ã‚¯å‡¦ç†ã®æœ€å¤§ä¸¦åˆ—ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 3ï¼‰
            register_workers: ç™»éŒ²å‡¦ç†ã®æœ€å¤§ä¸¦åˆ—ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 2ï¼‰

        Returns:
            RegisterResult: ç™»éŒ²çµæœ
        """
        # ã‚¤ãƒ³ãƒ•ãƒ©åˆæœŸåŒ–ï¼ˆãƒ“ã‚¸ãƒã‚¹ãƒ­ã‚¸ãƒƒã‚¯å®Ÿè¡Œã®å‰ææ¡ä»¶ï¼‰
        self._logger.info("ğŸ—ï¸ Graphitiã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ§‹ç¯‰ä¸­...")
        await self._episode_repository.initialize()

        self._logger.info(
            f"ğŸ“ ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆç™»éŒ²é–‹å§‹ï¼ˆ2æ®µéšä¸¦åˆ—å‡¦ç†ï¼‰ - group_id: {group_id.value}, directory: {directory}"
        )
        self._logger.info(
            f"âš™ï¸ ä¸¦åˆ—å‡¦ç†è¨­å®š - ãƒãƒ£ãƒ³ã‚¯å‡¦ç†: {max_workers}ãƒ¯ãƒ¼ã‚«ãƒ¼, ç™»éŒ²å‡¦ç†: {register_workers}ãƒ¯ãƒ¼ã‚«ãƒ¼"
        )

        # 1. ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§å–å¾—ï¼ˆinput/ã¨work/ã®ä¸¡æ–¹ã‹ã‚‰ï¼‰
        input_directory = directory if "/input" in directory else f"{directory}/input"
        work_directory = (
            directory.replace("/input", "/work")
            if "/input" in directory
            else f"{directory}/work"
        )

        # input/ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ•ã‚¡ã‚¤ãƒ«
        input_files = []
        if Path(input_directory).exists():
            input_files = self._file_reader.list_supported_files(input_directory)

        # work/ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ•ã‚¡ã‚¤ãƒ«
        work_files = []
        if Path(work_directory).exists():
            work_files = self._file_reader.list_supported_files(work_directory)

        file_paths = input_files + work_files
        self._logger.info(
            f"ğŸ“„ å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(file_paths)} (input: {len(input_files)}, work: {len(work_files)})"
        )

        # 2. ãƒ•ã‚¡ã‚¤ãƒ«ç§»å‹•ã®ãŸã‚ã®åŸºæº–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’è¨­å®š
        self._file_reader._base_directory = directory

        # 3. ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ï¼ˆåŸºæº–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’æŒ‡å®šã—ã¦ç›¸å¯¾ãƒ‘ã‚¹è¨ˆç®—ï¼‰
        documents = self._file_reader.read_documents(file_paths, directory)

        self._logger.info(f"ğŸ“– èª­ã¿è¾¼ã¿å®Œäº†ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(documents)}")

        if not documents:
            return RegisterResult(
                total_files=0, total_chunks=0, total_episodes=0, success=True
            )

        # 3. æœ€é©ãªãƒ¯ãƒ¼ã‚«ãƒ¼æ•°ã‚’æ±ºå®š
        optimal_workers = self._determine_optimal_workers(documents, max_workers)

        # 4. ä¸¦åˆ—å‡¦ç†ã§ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå‡¦ç†ï¼ˆã‚¨ãƒ©ãƒ¼å†å‡¦ç†å¯¾å¿œï¼‰
        all_episodes = []
        total_chunks = 0
        failed_files = []

        # ãƒ•ã‚¡ã‚¤ãƒ«ã®å ´æ‰€ã«å¿œã˜ã¦å‡¦ç†æ–¹æ³•ã‚’æ±ºå®š
        documents_to_process = []
        documents_in_work = []

        for doc in documents:
            # work/ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ•ã‚¡ã‚¤ãƒ«ã¯æ—¢ã«ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã™ã‚‹ã¯ãš
            if "/work/" in doc.file_path:
                # work/ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ•ã‚¡ã‚¤ãƒ«ã¯æ—¢ã«ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰åŒ–æ¸ˆã¿
                if self._chunk_file_manager.has_saved_episodes(doc.file_path):
                    documents_in_work.append(doc)
                    self._logger.info(f"ğŸ”„ å‡¦ç†ä¸­ãƒ•ã‚¡ã‚¤ãƒ«ç™ºè¦‹: {doc.file_name} (work/)")
                else:
                    # ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ãƒ•ã‚¡ã‚¤ãƒ«ãŒãªã„å ´åˆã¯é€šå¸¸å‡¦ç†ã«æˆ»ã™
                    documents_to_process.append(doc)
                    self._logger.warning(
                        f"âš ï¸ ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ãƒ•ã‚¡ã‚¤ãƒ«ãªã—: {doc.file_name} (work/) - å†å‡¦ç†ã—ã¾ã™"
                    )
            else:
                # é€šå¸¸ã®å‡¦ç†å¯¾è±¡
                documents_to_process.append(doc)

        # é€šå¸¸ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå‡¦ç†ï¼ˆãƒãƒ£ãƒ³ã‚¯ç”Ÿæˆã‹ã‚‰é–‹å§‹ï¼‰
        if documents_to_process:
            with ThreadPoolExecutor(max_workers=optimal_workers) as executor:
                # ä¸¦åˆ—å®Ÿè¡Œã®ãŸã‚ã®ã‚¿ã‚¹ã‚¯ã‚’æº–å‚™ï¼ˆçµ±ä¸€å‡¦ç†ãƒ•ãƒ­ãƒ¼ä½¿ç”¨ï¼‰
                future_to_doc = {
                    executor.submit(
                        self._process_single_document_with_recovery,
                        doc,
                        group_id,
                        i,
                        len(documents_to_process),
                        directory,  # åŸºæº–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’æ¸¡ã™
                    ): doc
                    for i, doc in enumerate(documents_to_process, 1)
                }

                # å®Œäº†ã—ãŸã‚¿ã‚¹ã‚¯ã‹ã‚‰çµæœã‚’å–å¾—
                for future in as_completed(future_to_doc):
                    episodes, chunks, error_file = future.result()

                    if error_file:
                        failed_files.append(error_file)
                    else:
                        all_episodes.extend(episodes)
                        total_chunks += chunks

        # 5. è¨­å®šèª­ã¿è¾¼ã¿
        config = load_config()
        batch_size = config.parallel.episode_batch_size

        # work/ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚’èª­ã¿è¾¼ã‚€
        if documents_in_work:
            self._logger.info(
                f"ğŸ“ work/ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†: {len(documents_in_work)}ãƒ•ã‚¡ã‚¤ãƒ«"
            )

            # work/ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ•ã‚¡ã‚¤ãƒ«ã¯ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚’èª­ã¿è¾¼ã‚€ã ã‘
            work_episodes_by_file = {}
            for doc in documents_in_work:
                try:
                    # ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿
                    episodes = self._chunk_file_manager.load_episodes(doc.file_path)
                    if episodes:
                        work_episodes_by_file[doc.file_path] = episodes
                        self._logger.info(
                            f"ğŸ“ ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰èª­ã¿è¾¼ã¿å®Œäº†: {doc.file_name} ({len(episodes)}ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰)"
                        )
                except Exception as e:
                    self._logger.error(
                        f"âŒ ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿å¤±æ•—: {doc.file_path} - {e}"
                    )
                    failed_files.append(doc.file_path)

            # work/ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚‚ä¿å­˜å¯¾è±¡ã«è¿½åŠ 
            if work_episodes_by_file:
                # æ—¢å­˜ã®å‡¦ç†ã¨çµ±åˆã™ã‚‹ãŸã‚ã€documents_to_processã«è¿½åŠ 
                # ï¼ˆã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã¯æ—¢ã« work_episodes_by_file ã«ä¿å­˜ã•ã‚Œã¦ã„ã‚‹ï¼‰
                await self._save_work_episodes_with_progress(
                    work_episodes_by_file, batch_size, register_workers
                )

        # å…¨ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã®åˆ†å‰²ä¿å­˜ï¼ˆé€²æ—è¿½è·¡ä»˜ãï¼‰
        if all_episodes:
            self._logger.info(
                f"ğŸ’¾ ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰åˆ†å‰²ä¿å­˜é–‹å§‹ - ä»¶æ•°: {len(all_episodes)}, "
                f"ãƒãƒƒãƒã‚µã‚¤ã‚º: {batch_size}, ç™»éŒ²ãƒ¯ãƒ¼ã‚«ãƒ¼: {register_workers}"
            )
            save_start = time.time()

            try:
                await self._save_episodes_with_progress_tracking(
                    all_episodes, documents_to_process, batch_size, register_workers
                )
                save_time = time.time() - save_start
                self._logger.info(
                    f"âœ… ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰åˆ†å‰²ä¿å­˜å®Œäº† - ä¿å­˜æ™‚é–“: {save_time:.2f}ç§’"
                )
            except Exception as e:
                save_time = time.time() - save_start
                self._logger.error(
                    f"âŒ ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰åˆ†å‰²ä¿å­˜å¤±æ•— - ä¿å­˜æ™‚é–“: {save_time:.2f}ç§’, ã‚¨ãƒ©ãƒ¼: {e}"
                )
                # ã‚¨ãƒ©ãƒ¼ã§ã‚‚å‡¦ç†ã‚’ç¶™ç¶šï¼ˆéƒ¨åˆ†ä¿å­˜ã•ã‚ŒãŸã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã¯é€²æ—ã«è¨˜éŒ²æ¸ˆã¿ï¼‰

        # 6. å‡¦ç†å®Œäº†ãƒ•ã‚¡ã‚¤ãƒ«ã‚’doneãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ç§»å‹•
        # æ³¨: æ–°æ–¹å¼ã§ã¯workâ†’doneã®ç§»å‹•ã¯ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ç™»éŒ²æ™‚ã«å€‹åˆ¥ã«å®Ÿè¡Œã•ã‚Œã‚‹ãŸã‚ã€
        # ã“ã“ã§ã¯äº’æ›æ€§ã®ãŸã‚æ®‹å­˜ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒã‚§ãƒƒã‚¯ã®ã¿è¡Œã†
        done_directory = (
            directory.replace("/input", "/done")
            if "/input" in directory
            else f"{directory}/done"
        )
        successful_files = []

        # ã™ã¹ã¦ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ãƒã‚§ãƒƒã‚¯ï¼ˆäº’æ›æ€§ã®ãŸã‚ï¼‰
        all_processed_documents = documents_to_process

        for doc in all_processed_documents:
            if doc.file_path not in failed_files:
                try:
                    # æ—¢ã«done/ã«ç§»å‹•æ¸ˆã¿ã‹ãƒã‚§ãƒƒã‚¯
                    if "/done/" in doc.file_path:
                        successful_files.append(doc.file_path)
                        continue

                    # ãƒãƒ£ãƒ³ã‚¯ãƒ•ã‚¡ã‚¤ãƒ«ã¨æ®‹å­˜ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ãƒ•ã‚¡ã‚¤ãƒ«ãŒæ®‹ã£ã¦ã„ãªã„ã“ã¨ã‚’ç¢ºèª
                    has_chunks = self._chunk_file_manager.has_chunk_files(doc.file_path)
                    has_remaining_episodes = (
                        self._chunk_file_manager.has_saved_episodes(doc.file_path)
                    )

                    if not has_chunks and not has_remaining_episodes:
                        # å¤ã„æ–¹å¼ã®äº’æ›æ€§ã®ãŸã‚ã€input/ã®ãƒ•ã‚¡ã‚¤ãƒ«ã¯done/ã«ç§»å‹•
                        if "/input/" in doc.file_path:
                            moved_path = self._file_reader.move_file(
                                doc.file_path, done_directory
                            )
                            successful_files.append(moved_path)
                            self._logger.info(
                                f"ğŸ“ å®Œäº†ãƒ•ã‚¡ã‚¤ãƒ«ç§»å‹•: {doc.file_name} â†’ done/"
                            )
                        else:
                            successful_files.append(doc.file_path)
                    else:
                        skip_reasons = []
                        if has_chunks:
                            skip_reasons.append("ãƒãƒ£ãƒ³ã‚¯ãƒ•ã‚¡ã‚¤ãƒ«æ®‹å­˜")
                        if has_remaining_episodes:
                            skip_reasons.append("æœªå‡¦ç†ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰æ®‹å­˜")

                        self._logger.debug(
                            f"âš ï¸ ãƒ•ã‚¡ã‚¤ãƒ«ç§»å‹•ã‚¹ã‚­ãƒƒãƒ—: {doc.file_name} ({', '.join(skip_reasons)})"
                        )
                except Exception as e:
                    self._logger.warning(
                        f"âš ï¸ ãƒ•ã‚¡ã‚¤ãƒ«ç§»å‹•ç¢ºèªå¤±æ•—: {doc.file_name} - {e}"
                    )

        if failed_files:
            self._logger.warning(f"âš ï¸ å‡¦ç†å¤±æ•—ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(failed_files)}")
            for failed_file in failed_files:
                self._logger.warning(f"  - {failed_file}")

        if successful_files:
            self._logger.info(f"âœ… å®Œäº†ãƒ•ã‚¡ã‚¤ãƒ«ç§»å‹•æ•°: {len(successful_files)}")

        # ã‚¨ãƒ©ãƒ¼å†å‡¦ç†ã®çµ±è¨ˆæƒ…å ±ã‚’å‡ºåŠ›
        cache_stats = self._chunk_file_manager.get_cache_stats()
        if cache_stats["total_cached_files"] > 0:
            self._logger.info(
                f"ğŸ“Š ãƒãƒ£ãƒ³ã‚¯ã‚­ãƒ£ãƒƒã‚·ãƒ¥: {cache_stats['total_cached_files']}ãƒ•ã‚¡ã‚¤ãƒ«, "
                f"{cache_stats['total_chunks']}ãƒãƒ£ãƒ³ã‚¯, {cache_stats['total_size_mb']}MB"
            )

        return RegisterResult(
            total_files=len(all_processed_documents),
            total_chunks=total_chunks,
            total_episodes=len(all_episodes),
            success=len(failed_files) == 0,  # å¤±æ•—ãƒ•ã‚¡ã‚¤ãƒ«ãŒãªã„å ´åˆã®ã¿æˆåŠŸ
            error_message=f"{len(failed_files)}å€‹ã®ãƒ•ã‚¡ã‚¤ãƒ«ã§å‡¦ç†ã«å¤±æ•—ã—ã¾ã—ãŸ"
            if failed_files
            else "",
        )

    async def _save_episodes_with_progress_tracking(
        self,
        all_episodes: List,
        documents: List,
        batch_size: int,
        max_concurrent: int,
    ) -> None:
        """
        ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚’åˆ†å‰²ä¿å­˜ã—ã€é€²æ—ã‚’è¿½è·¡ã™ã‚‹

        Args:
            all_episodes: ä¿å­˜ã™ã‚‹ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã®ãƒªã‚¹ãƒˆ
            documents: ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®ãƒªã‚¹ãƒˆ
            batch_size: ãƒãƒƒãƒã‚µã‚¤ã‚º
            max_concurrent: æœ€å¤§åŒæ™‚å®Ÿè¡Œæ•°

        Raises:
            Exception: ä¿å­˜ã«å¤±æ•—ã—ãŸå ´åˆ
        """
        # ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚’ãƒ•ã‚¡ã‚¤ãƒ«åˆ¥ã«ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
        episodes_by_file = {}
        for episode in all_episodes:
            # ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰åã‹ã‚‰ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’æŠ½å‡ºï¼ˆä»®ã®å®Ÿè£…ï¼‰
            source_file = self._extract_source_file_from_episode(episode, documents)
            if source_file not in episodes_by_file:
                episodes_by_file[source_file] = []
            episodes_by_file[source_file].append(episode)

        # ãƒ•ã‚¡ã‚¤ãƒ«ã”ã¨ã«åˆ†å‰²ä¿å­˜ã‚’å®Ÿè¡Œ
        for file_path, file_episodes in episodes_by_file.items():
            await self._save_file_episodes_with_progress(
                file_path, file_episodes, batch_size, max_concurrent
            )

    async def _save_file_episodes_with_progress(
        self,
        file_path: str,
        file_episodes: List,
        batch_size: int,  # äº’æ›æ€§ã®ãŸã‚ä¿æŒï¼ˆã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰å˜ä½ä¿å­˜ã§ã¯æœªä½¿ç”¨ï¼‰
        max_concurrent: int,
    ) -> None:
        """
        å˜ä¸€ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚’ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰å˜ä½ã§ä¿å­˜ã—ã€é€²æ—ã‚’è¿½è·¡ã™ã‚‹ï¼ˆäº‹å‰ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜æ–¹å¼ï¼‰

        Args:
            file_path: ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
            file_episodes: ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ãƒªã‚¹ãƒˆ
            batch_size: ãƒãƒƒãƒã‚µã‚¤ã‚ºï¼ˆäº’æ›æ€§ã®ãŸã‚ä¿æŒã€ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰å˜ä½ä¿å­˜ã§ã¯æœªä½¿ç”¨ï¼‰
            max_concurrent: æœ€å¤§åŒæ™‚å®Ÿè¡Œæ•°

        Raises:
            Exception: ä¿å­˜ã«å¤±æ•—ã—ãŸå ´åˆ
        """
        # batch_sizeã¯äº’æ›æ€§ã®ãŸã‚ä¿æŒã—ã¦ã„ã‚‹ãŒã€ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰å˜ä½ä¿å­˜ã§ã¯ä½¿ç”¨ã—ãªã„
        _ = batch_size
        total_episodes = len(file_episodes)

        # äº‹å‰ã«å…¨ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚’ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜ã™ã‚‹ï¼ˆStage 1: äº‹å‰ä¿å­˜ï¼‰
        if not self._chunk_file_manager.has_saved_episodes(file_path):
            self._logger.info(
                f"ğŸ’¾ äº‹å‰ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ä¿å­˜é–‹å§‹: {file_path} ({total_episodes}ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰)"
            )
            self._chunk_file_manager.save_episodes(file_path, file_episodes, 0)

            self._logger.info(f"âœ… äº‹å‰ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ä¿å­˜å®Œäº†: {file_path}")

        # ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰æ®‹å­˜åˆ†ã‚’ç¢ºèª
        start_index = 0

        # æ®‹å­˜ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰æœªå‡¦ç†åˆ†ã‚’ç‰¹å®š
        remaining_episodes = []
        for episode_index in range(start_index, total_episodes):
            episode_file = self._chunk_file_manager._get_episode_file_path(
                file_path, episode_index
            )
            if episode_file.exists():
                try:
                    # ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚’èª­ã¿è¾¼ã¿
                    episode_list = self._chunk_file_manager.load_episodes(
                        file_path, episode_index, episode_index
                    )
                    if episode_list:
                        remaining_episodes.append((episode_index, episode_list[0]))
                except Exception as e:
                    self._logger.warning(
                        f"âš ï¸ ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿å¤±æ•—: {episode_file} - {e}"
                    )

        if not remaining_episodes:
            self._logger.info(f"âœ… ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ä¿å­˜æ¸ˆã¿: {file_path}")
            return

        self._logger.info(
            f"ğŸ”„ ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ä¸¦åˆ—ä¿å­˜é–‹å§‹: {file_path} "
            f"({len(remaining_episodes)}/{total_episodes}ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰æ®‹ã‚Š)"
        )

        # Stage 2: ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰å˜ä½ã§ã®ä¸¦åˆ—ä¿å­˜
        await self._save_episodes_parallel_with_progress(
            file_path, remaining_episodes, max_concurrent, total_episodes
        )

        # å…¨ä¿å­˜å®Œäº†æ™‚
        self._logger.info(
            f"âœ… ãƒ•ã‚¡ã‚¤ãƒ«ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ä¿å­˜å®Œäº†: {file_path} ({total_episodes}ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰)"
        )

    async def _save_episodes_parallel_with_progress(
        self,
        file_path: str,
        remaining_episodes: List[Tuple[int, Any]],
        max_concurrent: int,
        total_episodes: int,
    ) -> None:
        """
        ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚’ä¸¦åˆ—ã§ä¿å­˜ã—ã€é€²æ—ã‚’è¿½è·¡ã™ã‚‹

        Args:
            file_path: ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
            remaining_episodes: æ®‹å­˜ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã®ãƒªã‚¹ãƒˆï¼ˆã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã€ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ï¼‰ã®ã‚¿ãƒ—ãƒ«
            max_concurrent: æœ€å¤§åŒæ™‚å®Ÿè¡Œæ•°
            total_episodes: ç·ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰æ•°

        Raises:
            Exception: ä¿å­˜ã«å¤±æ•—ã—ãŸå ´åˆ
        """
        import asyncio

        # ã‚»ãƒãƒ•ã‚©ã§åŒæ™‚å®Ÿè¡Œæ•°ã‚’åˆ¶é™
        semaphore = asyncio.Semaphore(max_concurrent)

        async def save_single_with_semaphore(episode_index: int, episode):
            async with semaphore:
                await self._save_single_episode_with_progress(
                    file_path, episode_index, episode, total_episodes
                )

        # å…¨ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã®ä¿å­˜ã‚¿ã‚¹ã‚¯ã‚’ä½œæˆ
        tasks = []
        for episode_index, episode in remaining_episodes:
            task = save_single_with_semaphore(episode_index, episode)
            tasks.append(task)

        # ä¸¦åˆ—å®Ÿè¡Œï¼ˆä¾‹å¤–ãŒç™ºç”Ÿã—ã¦ã‚‚ä»–ã®ã‚¿ã‚¹ã‚¯ã‚’ç¶™ç¶šï¼‰
        results = await asyncio.gather(*tasks, return_exceptions=True)

        # çµæœã‚’ç¢ºèªã—ã€ã‚¨ãƒ©ãƒ¼ãŒã‚ã‚Œã°ãƒ­ã‚°å‡ºåŠ›
        error_count = 0
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                episode_index, episode = remaining_episodes[i]
                self._logger.error(
                    f"âŒ ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ä¿å­˜å¤±æ•—: {file_path} [index:{episode_index}] {episode.name} - {result}"
                )
                error_count += 1

        if error_count > 0:
            self._logger.warning(
                f"âš ï¸ ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ä¿å­˜ã§{error_count}ä»¶ã®ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {file_path}"
            )
        else:
            self._logger.info(f"âœ… å…¨ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ä¿å­˜æˆåŠŸ: {file_path}")

    async def _save_single_episode_with_progress(
        self,
        file_path: str,
        episode_index: int,
        episode: Any,
        total_episodes: int,
    ) -> None:
        """
        å˜ä¸€ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚’ä¿å­˜ã—ã€æˆåŠŸæ™‚ã«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤ã—ã¦é€²æ—ã‚’æ›´æ–°ã™ã‚‹

        Args:
            file_path: ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
            episode_index: ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
            episode: ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰
            total_episodes: ç·ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰æ•°

        Raises:
            Exception: ä¿å­˜ã«å¤±æ•—ã—ãŸå ´åˆ
        """
        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’è¨­å®š
        from src.adapter.logging_utils import current_file

        current_file.set(file_path)

        self._logger.debug(
            f"ğŸ“ ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ä¿å­˜ä¸­ [{episode_index}/{total_episodes - 1}]: {episode.name}"
        )

        # ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚’ä¿å­˜ï¼ˆã“ã“ã§ä¾‹å¤–ãŒç™ºç”Ÿã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ï¼‰
        await self._episode_repository.save(episode)

        # ã“ã“ã¾ã§åˆ°é” = ä¿å­˜æˆåŠŸ
        # æˆåŠŸæ™‚: å¯¾å¿œã™ã‚‹ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å³åº§ã«å‰Šé™¤
        self._chunk_file_manager.delete_episode_files(
            file_path, episode_index, episode_index
        )

        self._logger.debug(f"âœ… ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ä¿å­˜æˆåŠŸ [{episode_index}]: {episode.name}")

        # å…¨ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰å‡¦ç†å®Œäº†ãƒã‚§ãƒƒã‚¯ï¼ˆworkâ†’doneç§»å‹•ï¼‰
        if not self._chunk_file_manager.has_saved_episodes(file_path):
            # work/ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿ç§»å‹•å¯¾è±¡
            if "/work/" in file_path:
                done_directory = file_path.replace("/work/", "/done/")
                # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªéƒ¨åˆ†ã®ã¿æŠ½å‡º
                done_dir = str(Path(done_directory).parent)

                try:
                    self._file_reader.move_file(file_path, done_dir)
                    self._logger.info(
                        f"ğŸ“ å®Œäº†ãƒ•ã‚¡ã‚¤ãƒ«ç§»å‹•: {Path(file_path).name} â†’ done/"
                    )
                except FileNotFoundError:
                    # æ—¢ã«ä»–ã®ã‚¹ãƒ¬ãƒƒãƒ‰ãŒç§»å‹•æ¸ˆã¿
                    self._logger.debug(
                        f"ğŸ“ ãƒ•ã‚¡ã‚¤ãƒ«æ—¢ã«ç§»å‹•æ¸ˆã¿: {Path(file_path).name}"
                    )
                except Exception as e:
                    self._logger.warning(
                        f"âš ï¸ ãƒ•ã‚¡ã‚¤ãƒ«ç§»å‹•å¤±æ•—: {Path(file_path).name} - {e}"
                    )

    async def _save_work_episodes_with_progress(
        self,
        work_episodes_by_file: Dict[str, List],
        batch_size: int,
        max_concurrent: int,
    ) -> None:
        """
        work/ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚’ä¿å­˜ã™ã‚‹

        Args:
            work_episodes_by_file: ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã¨ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ãƒªã‚¹ãƒˆã®è¾æ›¸
            batch_size: ãƒãƒƒãƒã‚µã‚¤ã‚ºï¼ˆäº’æ›æ€§ã®ãŸã‚ä¿æŒï¼‰
            max_concurrent: æœ€å¤§åŒæ™‚å®Ÿè¡Œæ•°
        """
        # ãƒ•ã‚¡ã‚¤ãƒ«ã”ã¨ã«ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ä¿å­˜ã‚’å®Ÿè¡Œ
        for file_path, file_episodes in work_episodes_by_file.items():
            await self._save_file_episodes_with_progress(
                file_path, file_episodes, batch_size, max_concurrent
            )

    def _extract_source_file_from_episode(self, episode, documents: List) -> str:
        """
        ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‹ã‚‰å…ƒãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’æŠ½å‡ºã™ã‚‹

        Args:
            episode: ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰
            documents: ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãƒªã‚¹ãƒˆ

        Returns:
            str: å…ƒãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
        """
        # ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰åã‹ã‚‰ãƒ•ã‚¡ã‚¤ãƒ«åã‚’æŠ½å‡ºï¼ˆä¾‹: "sample.pdf - chunk 0" -> "sample.pdf"ï¼‰
        episode_name = episode.name

        # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãƒªã‚¹ãƒˆã‹ã‚‰å¯¾å¿œã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’æ¢ã™
        for doc in documents:
            file_name = Path(doc.file_path).name
            if file_name in episode_name:
                return doc.file_path

        # è¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰åã‚’ãã®ã¾ã¾ä½¿ç”¨
        return episode_name.split(" - ")[0] if " - " in episode_name else episode_name
