"""RegisterDocumentUseCase - ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆç™»éŒ²ã®ãƒ¦ãƒ¼ã‚¹ã‚±ãƒ¼ã‚¹"""

import logging
import multiprocessing
import time
from collections import Counter
from dataclasses import dataclass
from typing import List, Tuple
from concurrent.futures import ThreadPoolExecutor, as_completed

from src.domain.group_id import GroupId
from src.adapter.filesystem_document_reader import FileSystemDocumentReader
from src.adapter.unstructured_document_parser import UnstructuredDocumentParser
from src.adapter.graphiti_episode_repository import GraphitiEpisodeRepository


@dataclass
class RegisterResult:
    """ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆç™»éŒ²çµæœ"""

    total_files: int
    total_chunks: int
    total_episodes: int
    success: bool
    error_message: str = ""


class RegisterDocumentUseCase:
    """ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆç™»éŒ²ã®ãƒ¦ãƒ¼ã‚¹ã‚±ãƒ¼ã‚¹"""

    def __init__(
        self,
        file_reader: FileSystemDocumentReader,
        document_parser: UnstructuredDocumentParser,
        episode_repository: GraphitiEpisodeRepository,
    ) -> None:
        """
        RegisterDocumentUseCaseã‚’åˆæœŸåŒ–ã™ã‚‹

        Args:
            file_reader: ãƒ•ã‚¡ã‚¤ãƒ«ã‚·ã‚¹ãƒ†ãƒ ã‹ã‚‰ã®èª­ã¿è¾¼ã¿
            document_parser: ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆè§£æã¨ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²
            episode_repository: ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ä¿å­˜
        """
        self._file_reader = file_reader
        self._document_parser = document_parser
        self._episode_repository = episode_repository
        self._logger = logging.getLogger(__name__)

    def _determine_optimal_workers(
        self, documents: List, requested_workers: int
    ) -> int:
        """
        ãƒ•ã‚¡ã‚¤ãƒ«ã‚¿ã‚¤ãƒ—ã«å¿œã˜ã¦æœ€é©ãªãƒ¯ãƒ¼ã‚«ãƒ¼æ•°ã‚’æ±ºå®šã™ã‚‹

        Args:
            documents: å‡¦ç†å¯¾è±¡ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãƒªã‚¹ãƒˆ
            requested_workers: ãƒªã‚¯ã‚¨ã‚¹ãƒˆã•ã‚ŒãŸãƒ¯ãƒ¼ã‚«ãƒ¼æ•°

        Returns:
            int: æœ€é©åŒ–ã•ã‚ŒãŸãƒ¯ãƒ¼ã‚«ãƒ¼æ•°
        """
        if not documents:
            return 1

        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚¿ã‚¤ãƒ—ã®çµ±è¨ˆã‚’å–å¾—
        file_types = [doc.file_type for doc in documents]
        type_counter = Counter(file_types)
        total_files = len(documents)

        # ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¿ã‚¤ãƒ—ï¼ˆCPUè² è·ãŒé«˜ã„ï¼‰
        image_types = {"png", "jpg", "jpeg", "bmp", "tiff", "tif", "heic"}
        image_count = sum(
            count
            for file_type, count in type_counter.items()
            if file_type.lower() in image_types
        )

        # PDF ãƒ•ã‚¡ã‚¤ãƒ«ã®æ•°ï¼ˆãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãŒå¤šã„ï¼‰
        pdf_count = type_counter.get("pdf", 0)

        # ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã®å‰²åˆ
        image_ratio = image_count / total_files
        pdf_ratio = pdf_count / total_files

        cpu_count = multiprocessing.cpu_count()

        # æœ€é©ãªãƒ¯ãƒ¼ã‚«ãƒ¼æ•°ã‚’æ±ºå®š
        if image_ratio > 0.5:  # ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ãŒ50%ä»¥ä¸Š
            optimal_workers = min(cpu_count // 2, total_files, 4)
            self._logger.info(
                f"ğŸ“Š ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°èª¿æ•´ - ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ç‡ {image_ratio:.1%}: "
                f"{requested_workers} â†’ {optimal_workers} ãƒ¯ãƒ¼ã‚«ãƒ¼"
            )
        elif pdf_ratio > 0.7:  # PDFãƒ•ã‚¡ã‚¤ãƒ«ãŒ70%ä»¥ä¸Š
            optimal_workers = min(cpu_count // 2, total_files, 6)
            self._logger.info(
                f"ğŸ“Š ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°èª¿æ•´ - PDFç‡ {pdf_ratio:.1%}: "
                f"{requested_workers} â†’ {optimal_workers} ãƒ¯ãƒ¼ã‚«ãƒ¼"
            )
        else:  # ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ãŒå¤šã„å ´åˆã¯ç©æ¥µçš„ã«ä¸¦åˆ—åŒ–
            optimal_workers = min(cpu_count, total_files, requested_workers)
            if optimal_workers != requested_workers:
                self._logger.info(
                    f"ğŸ“Š ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°èª¿æ•´ - è»½é‡ãƒ•ã‚¡ã‚¤ãƒ«ä¸­å¿ƒ: "
                    f"{requested_workers} â†’ {optimal_workers} ãƒ¯ãƒ¼ã‚«ãƒ¼"
                )

        # ãƒ•ã‚¡ã‚¤ãƒ«ç¨®åˆ¥ã®çµ±è¨ˆã‚’ãƒ­ã‚°å‡ºåŠ›
        self._logger.info(
            f"ğŸ“ˆ ãƒ•ã‚¡ã‚¤ãƒ«çµ±è¨ˆ - ç·æ•°: {total_files}, ç”»åƒ: {image_count}, "
            f"PDF: {pdf_count}, ãã®ä»–: {total_files - image_count - pdf_count}"
        )

        return max(1, optimal_workers)  # æœ€ä½1ãƒ¯ãƒ¼ã‚«ãƒ¼ã¯ç¢ºä¿

    def _process_single_document(
        self, document, group_id: GroupId, index: int, total: int
    ) -> Tuple[List, int, str]:
        """å˜ä¸€ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®å‡¦ç†ï¼ˆä¸¦åˆ—å®Ÿè¡Œç”¨ï¼‰"""
        from src.adapter.logging_utils import current_file

        # ç¾åœ¨å‡¦ç†ä¸­ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«è¨­å®š
        current_file.set(document.file_path)

        try:
            self._logger.info(
                f"ğŸ”„ ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ä¸­ ({index}/{total}): {document.file_name}"
            )

            # å…¨ä½“ã®å‡¦ç†æ™‚é–“è¨ˆæ¸¬é–‹å§‹
            start_time = time.time()

            # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’è§£æ
            parse_start = time.time()
            elements = self._document_parser.parse(document.file_path)
            parse_time = time.time() - parse_start
            self._logger.debug(f"ğŸ“ è§£æå®Œäº† - è¦ç´ æ•°: {len(elements)}")

            # ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²
            chunk_start = time.time()
            chunks = self._document_parser.split_elements(elements, document)
            chunk_time = time.time() - chunk_start
            self._logger.debug(f"ğŸ”€ ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²å®Œäº† - ãƒãƒ£ãƒ³ã‚¯æ•°: {len(chunks)}")

            # ãƒãƒ£ãƒ³ã‚¯ã‹ã‚‰ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚’ä½œæˆ
            episode_start = time.time()
            episodes = []
            for j, chunk in enumerate(chunks):
                episode = chunk.to_episode(group_id)
                episodes.append(episode)
                self._logger.debug(
                    f"ğŸ“‹ ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ä½œæˆ ({j + 1}/{len(chunks)}): {episode.name}"
                )
            episode_time = time.time() - episode_start

            # å…¨ä½“ã®å‡¦ç†æ™‚é–“
            total_time = time.time() - start_time

            # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æƒ…å ±ã‚’ãƒ­ã‚°å‡ºåŠ›
            self._logger.info(
                f"â±ï¸ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ - {document.file_name} ({document.file_type}): "
                f"è§£æ {parse_time:.2f}ç§’, ãƒãƒ£ãƒ³ã‚¯åˆ†å‰² {chunk_time:.2f}ç§’, "
                f"ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ä½œæˆ {episode_time:.2f}ç§’, åˆè¨ˆ {total_time:.2f}ç§’"
            )

            return episodes, len(chunks), None

        except Exception as e:
            error_msg = f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†å¤±æ•—: {document.file_path} - {e}"
            self._logger.error(error_msg)
            return [], 0, document.file_path

    async def execute_parallel(
        self,
        group_id: GroupId,
        directory: str,
        max_workers: int = 3,
        register_workers: int = 2,
    ) -> RegisterResult:
        """
        ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆç™»éŒ²ã®å®Ÿè¡Œï¼ˆ2æ®µéšä¸¦åˆ—å‡¦ç†ç‰ˆï¼‰

        Args:
            group_id: ã‚°ãƒ«ãƒ¼ãƒ—ID
            directory: å¯¾è±¡ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
            max_workers: ãƒãƒ£ãƒ³ã‚¯å‡¦ç†ã®æœ€å¤§ä¸¦åˆ—ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 3ï¼‰
            register_workers: ç™»éŒ²å‡¦ç†ã®æœ€å¤§ä¸¦åˆ—ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 2ï¼‰

        Returns:
            RegisterResult: ç™»éŒ²çµæœ
        """
        # ã‚¤ãƒ³ãƒ•ãƒ©åˆæœŸåŒ–ï¼ˆãƒ“ã‚¸ãƒã‚¹ãƒ­ã‚¸ãƒƒã‚¯å®Ÿè¡Œã®å‰ææ¡ä»¶ï¼‰
        self._logger.info("ğŸ—ï¸ Graphitiã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ§‹ç¯‰ä¸­...")
        await self._episode_repository.initialize()

        self._logger.info(
            f"ğŸ“ ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆç™»éŒ²é–‹å§‹ï¼ˆ2æ®µéšä¸¦åˆ—å‡¦ç†ï¼‰ - group_id: {group_id.value}, directory: {directory}"
        )
        self._logger.info(
            f"âš™ï¸ ä¸¦åˆ—å‡¦ç†è¨­å®š - ãƒãƒ£ãƒ³ã‚¯å‡¦ç†: {max_workers}ãƒ¯ãƒ¼ã‚«ãƒ¼, ç™»éŒ²å‡¦ç†: {register_workers}ãƒ¯ãƒ¼ã‚«ãƒ¼"
        )

        # 1. ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§å–å¾—
        file_paths = self._file_reader.list_supported_files(directory)
        self._logger.info(f"ğŸ“„ å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(file_paths)}")

        # 2. ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ï¼ˆåŸºæº–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’æŒ‡å®šã—ã¦ç›¸å¯¾ãƒ‘ã‚¹è¨ˆç®—ï¼‰
        documents = self._file_reader.read_documents(file_paths, directory)

        self._logger.info(f"ğŸ“– èª­ã¿è¾¼ã¿å®Œäº†ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(documents)}")

        if not documents:
            return RegisterResult(
                total_files=0, total_chunks=0, total_episodes=0, success=True
            )

        # 3. æœ€é©ãªãƒ¯ãƒ¼ã‚«ãƒ¼æ•°ã‚’æ±ºå®š
        optimal_workers = self._determine_optimal_workers(documents, max_workers)

        # 4. ä¸¦åˆ—å‡¦ç†ã§ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå‡¦ç†
        all_episodes = []
        total_chunks = 0
        failed_files = []

        with ThreadPoolExecutor(max_workers=optimal_workers) as executor:
            # ä¸¦åˆ—å®Ÿè¡Œã®ãŸã‚ã®ã‚¿ã‚¹ã‚¯ã‚’æº–å‚™
            future_to_doc = {
                executor.submit(
                    self._process_single_document, doc, group_id, i, len(documents)
                ): doc
                for i, doc in enumerate(documents, 1)
            }

            # å®Œäº†ã—ãŸã‚¿ã‚¹ã‚¯ã‹ã‚‰çµæœã‚’å–å¾—
            for future in as_completed(future_to_doc):
                episodes, chunks, error_file = future.result()

                if error_file:
                    failed_files.append(error_file)
                else:
                    all_episodes.extend(episodes)
                    total_chunks += chunks

        # 5. ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚’ä¸€æ‹¬ä¿å­˜ï¼ˆç™»éŒ²ç”¨ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°ã§åˆ¶å¾¡ï¼‰
        if all_episodes:
            self._logger.info(
                f"ğŸ’¾ ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ä¸€æ‹¬ä¿å­˜é–‹å§‹ - ä»¶æ•°: {len(all_episodes)}, ç™»éŒ²ãƒ¯ãƒ¼ã‚«ãƒ¼: {register_workers}"
            )
            save_start = time.time()
            await self._episode_repository.save_batch(
                all_episodes, max_concurrent=register_workers
            )
            save_time = time.time() - save_start
            self._logger.info(
                f"âœ… ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ä¸€æ‹¬ä¿å­˜å®Œäº† - ä¿å­˜æ™‚é–“: {save_time:.2f}ç§’, ç™»éŒ²ãƒ¯ãƒ¼ã‚«ãƒ¼: {register_workers}"
            )

        if failed_files:
            self._logger.warning(f"âš ï¸ å‡¦ç†å¤±æ•—ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(failed_files)}")

        return RegisterResult(
            total_files=len(documents),
            total_chunks=total_chunks,
            total_episodes=len(all_episodes),
            success=True,
        )
