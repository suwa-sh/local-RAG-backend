"""RegisterDocumentUseCase - ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆç™»éŒ²ã®ãƒ¦ãƒ¼ã‚¹ã‚±ãƒ¼ã‚¹"""

import logging
from dataclasses import dataclass
from typing import List, Tuple
from concurrent.futures import ThreadPoolExecutor, as_completed

from src.domain.group_id import GroupId
from src.adapter.filesystem_document_reader import FileSystemDocumentReader
from src.adapter.unstructured_document_parser import UnstructuredDocumentParser
from src.adapter.graphiti_episode_repository import GraphitiEpisodeRepository


@dataclass
class RegisterResult:
    """ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆç™»éŒ²çµæœ"""

    total_files: int
    total_chunks: int
    total_episodes: int
    success: bool
    error_message: str = ""


class RegisterDocumentUseCase:
    """ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆç™»éŒ²ã®ãƒ¦ãƒ¼ã‚¹ã‚±ãƒ¼ã‚¹"""

    def __init__(
        self,
        file_reader: FileSystemDocumentReader,
        document_parser: UnstructuredDocumentParser,
        episode_repository: GraphitiEpisodeRepository,
    ) -> None:
        """
        RegisterDocumentUseCaseã‚’åˆæœŸåŒ–ã™ã‚‹

        Args:
            file_reader: ãƒ•ã‚¡ã‚¤ãƒ«ã‚·ã‚¹ãƒ†ãƒ ã‹ã‚‰ã®èª­ã¿è¾¼ã¿
            document_parser: ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆè§£æã¨ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²
            episode_repository: ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ä¿å­˜
        """
        self._file_reader = file_reader
        self._document_parser = document_parser
        self._episode_repository = episode_repository
        self._logger = logging.getLogger(__name__)

    async def execute(self, group_id: GroupId, directory: str) -> RegisterResult:
        """
        æŒ‡å®šãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ç™»éŒ²ã™ã‚‹

        Args:
            group_id: ã‚°ãƒ«ãƒ¼ãƒ—ID
            directory: å¯¾è±¡ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãƒ‘ã‚¹

        Returns:
            RegisterResult: ç™»éŒ²çµæœ

        Raises:
            FileNotFoundError: ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒå­˜åœ¨ã—ãªã„å ´åˆ
            Exception: ç™»éŒ²å‡¦ç†ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸå ´åˆ
        """
        self._logger.info(
            f"ğŸ“ ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆç™»éŒ²é–‹å§‹ - group_id: {group_id.value}, directory: {directory}"
        )

        # 1. ã‚µãƒãƒ¼ãƒˆå¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§ã‚’å–å¾—
        file_paths = self._file_reader.list_supported_files(directory)
        self._logger.info(f"ğŸ“„ å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(file_paths)}")

        # 2. ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’èª­ã¿è¾¼ã¿
        documents = self._file_reader.read_documents(file_paths)
        self._logger.info(f"ğŸ“– èª­ã¿è¾¼ã¿å®Œäº†ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(documents)}")

        if not documents:
            return RegisterResult(
                total_files=0, total_chunks=0, total_episodes=0, success=True
            )

        # 3. å„ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’å‡¦ç†ã—ã¦ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚’ä½œæˆ
        all_episodes = []
        total_chunks = 0
        failed_files = []

        for i, document in enumerate(documents, 1):
            try:
                self._logger.info(
                    f"ğŸ”„ ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ä¸­ ({i}/{len(documents)}): {document.file_name}"
                )

                # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’è§£æ
                elements = self._document_parser.parse(document.file_path)
                self._logger.debug(f"ğŸ“ è§£æå®Œäº† - è¦ç´ æ•°: {len(elements)}")

                # ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²
                chunks = self._document_parser.split_elements(elements, document)
                self._logger.debug(f"ğŸ”€ ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²å®Œäº† - ãƒãƒ£ãƒ³ã‚¯æ•°: {len(chunks)}")

                # ãƒãƒ£ãƒ³ã‚¯ã‹ã‚‰ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚’ä½œæˆ
                episodes = []
                for j, chunk in enumerate(chunks):
                    episode = chunk.to_episode(group_id)
                    episodes.append(episode)
                    self._logger.debug(
                        f"ğŸ“‹ ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ä½œæˆ ({j + 1}/{len(chunks)}): {episode.name}"
                    )

                all_episodes.extend(episodes)
                total_chunks += len(chunks)

            except Exception as e:
                failed_files.append(document.file_path)
                self._logger.error(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†å¤±æ•—: {document.file_path} - {e}")
                continue

        # 4. ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚’ä¸€æ‹¬ä¿å­˜
        if all_episodes:
            self._logger.info(f"ğŸ’¾ ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ä¸€æ‹¬ä¿å­˜é–‹å§‹ - ä»¶æ•°: {len(all_episodes)}")
            await self._episode_repository.save_batch(all_episodes)
            self._logger.info("âœ… ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ä¸€æ‹¬ä¿å­˜å®Œäº†")

        return RegisterResult(
            total_files=len(documents),
            total_chunks=total_chunks,
            total_episodes=len(all_episodes),
            success=True,
        )

    def _process_single_document(
        self, document, group_id: GroupId, index: int, total: int
    ) -> Tuple[List, int, str]:
        """å˜ä¸€ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®å‡¦ç†ï¼ˆä¸¦åˆ—å®Ÿè¡Œç”¨ï¼‰"""
        from src.adapter.logging_utils import current_file

        # ç¾åœ¨å‡¦ç†ä¸­ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«è¨­å®š
        current_file.set(document.file_path)

        try:
            self._logger.info(
                f"ğŸ”„ ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ä¸­ ({index}/{total}): {document.file_name}"
            )

            # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’è§£æ
            elements = self._document_parser.parse(document.file_path)
            self._logger.debug(f"ğŸ“ è§£æå®Œäº† - è¦ç´ æ•°: {len(elements)}")

            # ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²
            chunks = self._document_parser.split_elements(elements, document)
            self._logger.debug(f"ğŸ”€ ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²å®Œäº† - ãƒãƒ£ãƒ³ã‚¯æ•°: {len(chunks)}")

            # ãƒãƒ£ãƒ³ã‚¯ã‹ã‚‰ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚’ä½œæˆ
            episodes = []
            for j, chunk in enumerate(chunks):
                episode = chunk.to_episode(group_id)
                episodes.append(episode)
                self._logger.debug(
                    f"ğŸ“‹ ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ä½œæˆ ({j + 1}/{len(chunks)}): {episode.name}"
                )

            return episodes, len(chunks), None

        except Exception as e:
            error_msg = f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†å¤±æ•—: {document.file_path} - {e}"
            self._logger.error(error_msg)
            return [], 0, document.file_path

    async def execute_parallel(
        self, group_id: GroupId, directory: str, max_workers: int = 3
    ) -> RegisterResult:
        """
        ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆç™»éŒ²ã®å®Ÿè¡Œï¼ˆä¸¦åˆ—å‡¦ç†ç‰ˆï¼‰

        Args:
            group_id: ã‚°ãƒ«ãƒ¼ãƒ—ID
            directory: å¯¾è±¡ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
            max_workers: æœ€å¤§ä¸¦åˆ—ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 3ï¼‰

        Returns:
            RegisterResult: ç™»éŒ²çµæœ
        """
        self._logger.info(
            f"ğŸ“ ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆç™»éŒ²é–‹å§‹ï¼ˆä¸¦åˆ—å‡¦ç†ï¼‰ - group_id: {group_id.value}, directory: {directory}"
        )

        # 1. ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§å–å¾—
        file_paths = self._file_reader.list_supported_files(directory)
        self._logger.info(f"ğŸ“„ å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(file_paths)}")

        # 2. ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿
        documents = self._file_reader.read_documents(file_paths)

        self._logger.info(f"ğŸ“– èª­ã¿è¾¼ã¿å®Œäº†ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(documents)}")

        if not documents:
            return RegisterResult(
                total_files=0, total_chunks=0, total_episodes=0, success=True
            )

        # 3. ä¸¦åˆ—å‡¦ç†ã§ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå‡¦ç†
        all_episodes = []
        total_chunks = 0
        failed_files = []

        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # ä¸¦åˆ—å®Ÿè¡Œã®ãŸã‚ã®ã‚¿ã‚¹ã‚¯ã‚’æº–å‚™
            future_to_doc = {
                executor.submit(
                    self._process_single_document, doc, group_id, i, len(documents)
                ): doc
                for i, doc in enumerate(documents, 1)
            }

            # å®Œäº†ã—ãŸã‚¿ã‚¹ã‚¯ã‹ã‚‰çµæœã‚’å–å¾—
            for future in as_completed(future_to_doc):
                episodes, chunks, error_file = future.result()

                if error_file:
                    failed_files.append(error_file)
                else:
                    all_episodes.extend(episodes)
                    total_chunks += chunks

        # 4. ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚’ä¸€æ‹¬ä¿å­˜
        if all_episodes:
            self._logger.info(f"ğŸ’¾ ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ä¸€æ‹¬ä¿å­˜é–‹å§‹ - ä»¶æ•°: {len(all_episodes)}")
            await self._episode_repository.save_batch(all_episodes)
            self._logger.info("âœ… ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ä¸€æ‹¬ä¿å­˜å®Œäº†")

        if failed_files:
            self._logger.warning(f"âš ï¸ å‡¦ç†å¤±æ•—ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(failed_files)}")

        return RegisterResult(
            total_files=len(documents),
            total_chunks=total_chunks,
            total_episodes=len(all_episodes),
            success=True,
        )
