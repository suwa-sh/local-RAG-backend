"""ChunkFileManager - ãƒãƒ£ãƒ³ã‚¯ãƒ•ã‚¡ã‚¤ãƒ«ã®ç®¡ç†"""

import json
import logging
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Any, Optional, Tuple

from src.domain.chunk import Chunk


class ChunkFileManager:
    """ãƒãƒ£ãƒ³ã‚¯ãƒ•ã‚¡ã‚¤ãƒ«ã®ä¿å­˜ãƒ»èª­ã¿è¾¼ã¿ãƒ»å‰Šé™¤ã‚’ç®¡ç†ã™ã‚‹ã‚¯ãƒ©ã‚¹"""

    def __init__(self, chunks_directory: str = "data/input_chunks") -> None:
        """
        ChunkFileManagerã‚’åˆæœŸåŒ–ã™ã‚‹

        Args:
            chunks_directory: ãƒãƒ£ãƒ³ã‚¯ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜ã™ã‚‹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        """
        self._chunks_directory = Path(chunks_directory)
        self._logger = logging.getLogger(__name__)

        # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆï¼ˆå­˜åœ¨ã—ãªã„å ´åˆï¼‰
        self._chunks_directory.mkdir(parents=True, exist_ok=True)

    def _get_chunk_directory(self, file_path: str) -> Path:
        """
        ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã«å¯¾å¿œã™ã‚‹ãƒãƒ£ãƒ³ã‚¯ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’å–å¾—ã™ã‚‹

        Args:
            file_path: å…ƒãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹

        Returns:
            Path: ãƒãƒ£ãƒ³ã‚¯ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ‘ã‚¹
        """
        # /input/ ã¾ãŸã¯ /input_work/ ã‹ã‚‰å§‹ã¾ã‚‹ç›¸å¯¾ãƒ‘ã‚¹ã‚’æŠ½å‡º
        relative_path = file_path
        for prefix in ["/input/", "/input_work/"]:
            if prefix in file_path:
                # æœ€å¾Œã«å‡ºç¾ã™ã‚‹ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ã®å¾Œã‚éƒ¨åˆ†ã‚’å–å¾—
                parts = file_path.split(prefix)
                if len(parts) >= 2:
                    relative_path = prefix.join(parts[-1:])
                break

        # data/input_chunks/ç›¸å¯¾ãƒ‘ã‚¹/
        return self._chunks_directory / relative_path

    def _get_metadata_file_path(self, file_path: str) -> Path:
        """
        ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ã‚’å–å¾—ã™ã‚‹

        Args:
            file_path: å…ƒãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹

        Returns:
            Path: ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
        """
        chunk_dir = self._get_chunk_directory(file_path)
        return chunk_dir / "metadata.json"

    def _get_chunk_file_path(self, file_path: str, position: int) -> Path:
        """
        ãƒãƒ£ãƒ³ã‚¯ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ã‚’å–å¾—ã™ã‚‹

        Args:
            file_path: å…ƒãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
            position: ãƒãƒ£ãƒ³ã‚¯ã®ä½ç½®

        Returns:
            Path: ãƒãƒ£ãƒ³ã‚¯ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
        """
        chunk_dir = self._get_chunk_directory(file_path)
        return chunk_dir / f"chunk_{position}.json"

    def _get_episode_file_path(self, file_path: str, episode_index: int) -> Path:
        """
        ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ã‚’å–å¾—ã™ã‚‹

        Args:
            file_path: å…ƒãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
            episode_index: ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹

        Returns:
            Path: ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
        """
        chunk_dir = self._get_chunk_directory(file_path)
        return chunk_dir / f"episode_{episode_index}.json"

    def has_chunk_files(self, file_path: str) -> bool:
        """
        æŒ‡å®šãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒãƒ£ãƒ³ã‚¯ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯ã™ã‚‹

        Args:
            file_path: å…ƒãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹

        Returns:
            bool: ãƒãƒ£ãƒ³ã‚¯ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã™ã‚‹å ´åˆTrue
        """
        metadata_file = self._get_metadata_file_path(file_path)
        return metadata_file.exists()

    def save_chunks(
        self,
        chunks: List[Chunk],
        file_path: str,
        last_processed_position: int = -1,
        error_message: str = "",
    ) -> None:
        """
        ãƒãƒ£ãƒ³ã‚¯ãƒªã‚¹ãƒˆã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ã™ã‚‹

        Args:
            chunks: ä¿å­˜ã™ã‚‹ãƒãƒ£ãƒ³ã‚¯ã®ãƒªã‚¹ãƒˆ
            file_path: å…ƒãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
            last_processed_position: æœ€å¾Œã«å‡¦ç†ã•ã‚ŒãŸä½ç½®ï¼ˆ-1ã®å ´åˆã¯æœªå‡¦ç†ï¼‰
            error_message: ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸

        Raises:
            OSError: ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜ã«å¤±æ•—ã—ãŸå ´åˆ
        """
        if not chunks:
            self._logger.warning(f"ä¿å­˜å¯¾è±¡ã®ãƒãƒ£ãƒ³ã‚¯ãŒã‚ã‚Šã¾ã›ã‚“: {file_path}")
            return

        chunk_dir = self._get_chunk_directory(file_path)
        chunk_dir.mkdir(parents=True, exist_ok=True)

        try:
            # å„ãƒãƒ£ãƒ³ã‚¯ã‚’å€‹åˆ¥ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
            for chunk in chunks:
                position = chunk.metadata.get("position", 0)
                chunk_file = self._get_chunk_file_path(file_path, position)

                with open(chunk_file, "w", encoding="utf-8") as f:
                    f.write(chunk.to_json())

            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜
            metadata = {
                "original_file": file_path,
                "total_chunks": len(chunks),
                "last_processed_position": last_processed_position,
                "created_at": datetime.now().isoformat(),
                "error_message": error_message,
            }

            metadata_file = self._get_metadata_file_path(file_path)
            with open(metadata_file, "w", encoding="utf-8") as f:
                json.dump(metadata, f, ensure_ascii=False, indent=2)

            self._logger.info(
                f"ğŸ’¾ ãƒãƒ£ãƒ³ã‚¯ä¿å­˜å®Œäº†: {file_path} "
                f"({len(chunks)}ãƒãƒ£ãƒ³ã‚¯, å‡¦ç†ä½ç½®: {last_processed_position})"
            )

        except OSError as e:
            self._logger.error(f"âŒ ãƒãƒ£ãƒ³ã‚¯ä¿å­˜å¤±æ•—: {file_path} - {e}")
            raise

    def load_chunks(self, file_path: str) -> Tuple[List[Chunk], Dict[str, Any]]:
        """
        ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã«å¯¾å¿œã™ã‚‹ãƒãƒ£ãƒ³ã‚¯ã‚’èª­ã¿è¾¼ã‚€

        Args:
            file_path: å…ƒãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹

        Returns:
            Tuple[List[Chunk], Dict[str, Any]]: (ãƒãƒ£ãƒ³ã‚¯ãƒªã‚¹ãƒˆ, ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿)

        Raises:
            FileNotFoundError: ãƒãƒ£ãƒ³ã‚¯ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ãªã„å ´åˆ
            OSError: ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ãŸå ´åˆ
            json.JSONDecodeError: JSONè§£æã«å¤±æ•—ã—ãŸå ´åˆ
        """
        metadata_file = self._get_metadata_file_path(file_path)

        if not metadata_file.exists():
            raise FileNotFoundError(f"ãƒãƒ£ãƒ³ã‚¯ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ã¾ã›ã‚“: {file_path}")

        try:
            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿
            with open(metadata_file, "r", encoding="utf-8") as f:
                metadata = json.load(f)

            # å„ãƒãƒ£ãƒ³ã‚¯ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿
            chunks = []
            total_chunks = metadata.get("total_chunks", 0)

            for position in range(total_chunks):
                chunk_file = self._get_chunk_file_path(file_path, position)

                if chunk_file.exists():
                    with open(chunk_file, "r", encoding="utf-8") as f:
                        chunk_json = f.read()
                        chunk = Chunk.from_json(chunk_json)
                        chunks.append(chunk)
                else:
                    self._logger.warning(
                        f"âš ï¸ ãƒãƒ£ãƒ³ã‚¯ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {chunk_file}"
                    )

            self._logger.info(
                f"ğŸ“ ãƒãƒ£ãƒ³ã‚¯èª­ã¿è¾¼ã¿å®Œäº†: {file_path} "
                f"({len(chunks)}/{total_chunks}ãƒãƒ£ãƒ³ã‚¯)"
            )

            return chunks, metadata

        except (OSError, json.JSONDecodeError) as e:
            self._logger.error(f"âŒ ãƒãƒ£ãƒ³ã‚¯èª­ã¿è¾¼ã¿å¤±æ•—: {file_path} - {e}")
            raise

    def delete_all_chunks(self, file_path: str) -> None:
        """
        æŒ‡å®šãƒ•ã‚¡ã‚¤ãƒ«ã®ã™ã¹ã¦ã®ãƒãƒ£ãƒ³ã‚¯ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤ã™ã‚‹

        Args:
            file_path: å…ƒãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
        """
        chunk_dir = self._get_chunk_directory(file_path)

        if chunk_dir.exists():
            try:
                # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã®ã™ã¹ã¦ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤
                for file in chunk_dir.iterdir():
                    if file.is_file():
                        file.unlink()

                # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªè‡ªä½“ã‚’å‰Šé™¤
                chunk_dir.rmdir()

                self._logger.info(f"ğŸ—‘ï¸ ãƒãƒ£ãƒ³ã‚¯ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå‰Šé™¤å®Œäº†: {chunk_dir}")

            except OSError as e:
                self._logger.warning(
                    f"âš ï¸ ãƒãƒ£ãƒ³ã‚¯ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå‰Šé™¤å¤±æ•—: {chunk_dir} - {e}"
                )

    def get_metadata(self, file_path: str) -> Optional[Dict[str, Any]]:
        """
        æŒ‡å®šãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã™ã‚‹

        Args:
            file_path: å…ƒãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹

        Returns:
            Optional[Dict[str, Any]]: ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ï¼ˆå­˜åœ¨ã—ãªã„å ´åˆã¯Noneï¼‰
        """
        metadata_file = self._get_metadata_file_path(file_path)

        if not metadata_file.exists():
            return None

        try:
            with open(metadata_file, "r", encoding="utf-8") as f:
                return json.load(f)
        except (OSError, json.JSONDecodeError) as e:
            self._logger.warning(f"âš ï¸ ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å¤±æ•—: {file_path} - {e}")
            return None

    def get_cache_stats(self) -> Dict[str, Any]:
        """
        ãƒãƒ£ãƒ³ã‚¯ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®çµ±è¨ˆæƒ…å ±ã‚’å–å¾—ã™ã‚‹

        Returns:
            Dict[str, Any]: çµ±è¨ˆæƒ…å ±
        """
        if not self._chunks_directory.exists():
            return {"total_cached_files": 0, "total_chunks": 0, "total_size_mb": 0.0}

        total_files = 0
        total_chunks = 0
        total_size = 0

        for chunk_dir in self._chunks_directory.iterdir():
            if chunk_dir.is_dir():
                total_files += 1

                dir_total_chunks, dir_total_size = self._aggregate_chunk_data(chunk_dir)
                total_chunks += dir_total_chunks
                total_size += dir_total_size

        return {
            "total_cached_files": total_files,
            "total_chunks": total_chunks,
            "total_size_mb": round(total_size / (1024 * 1024), 2),
        }

    def _aggregate_chunk_data(self, chunk_dir):
        total_chunks = 0
        total_size = 0
        for chunk_file in chunk_dir.iterdir():
            if chunk_file.is_file() and chunk_file.suffix == ".json":
                if chunk_file.name.startswith("chunk_"):
                    total_chunks += 1
                total_size += chunk_file.stat().st_size
        return total_chunks, total_size

    # ===== ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ãƒ‡ãƒ¼ã‚¿æ°¸ç¶šåŒ–ç®¡ç† =====

    def save_episodes(
        self,
        file_path: str,
        episodes: List,
        start_index: int = 0,
    ) -> None:
        """
        ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ãƒªã‚¹ãƒˆã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ã™ã‚‹

        Args:
            file_path: å…ƒãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
            episodes: ä¿å­˜ã™ã‚‹ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã®ãƒªã‚¹ãƒˆ
            start_index: é–‹å§‹ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹

        Raises:
            OSError: ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜ã«å¤±æ•—ã—ãŸå ´åˆ
        """
        if not episodes:
            self._logger.warning(f"ä¿å­˜å¯¾è±¡ã®ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ãŒã‚ã‚Šã¾ã›ã‚“: {file_path}")
            return

        chunk_dir = self._get_chunk_directory(file_path)
        chunk_dir.mkdir(parents=True, exist_ok=True)

        try:
            # å„ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚’å€‹åˆ¥ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
            for i, episode in enumerate(episodes):
                episode_index = start_index + i
                episode_file = self._get_episode_file_path(file_path, episode_index)

                # ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚’è¾æ›¸å½¢å¼ã«å¤‰æ›
                episode_data = {
                    "name": episode.name,
                    "body": episode.body,
                    "source_description": episode.source_description,
                    "reference_time": episode.reference_time.isoformat()
                    if episode.reference_time
                    else None,
                    "episode_type": episode.episode_type,
                    "group_id": episode.group_id.value,
                }

                with open(episode_file, "w", encoding="utf-8") as f:
                    json.dump(episode_data, f, ensure_ascii=False, indent=2)

            self._logger.info(
                f"ğŸ’¾ ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜å®Œäº†: {file_path} "
                f"({len(episodes)}ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰, ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹: {start_index}ã€œ{start_index + len(episodes) - 1})"
            )

        except OSError as e:
            self._logger.error(f"âŒ ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜å¤±æ•—: {file_path} - {e}")
            raise

    def load_episodes(
        self,
        file_path: str,
        start_index: int = 0,
        end_index: Optional[int] = None,
    ) -> List:
        """
        æŒ‡å®šç¯„å›²ã®ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€

        Args:
            file_path: å…ƒãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
            start_index: é–‹å§‹ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
            end_index: çµ‚äº†ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ï¼ˆNoneã®å ´åˆã¯å­˜åœ¨ã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ã¾ã§ï¼‰

        Returns:
            List: ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ãƒªã‚¹ãƒˆ

        Raises:
            FileNotFoundError: ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ãªã„å ´åˆ
            OSError: ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ãŸå ´åˆ
            json.JSONDecodeError: JSONè§£æã«å¤±æ•—ã—ãŸå ´åˆ
        """
        from src.domain.episode import Episode
        from src.domain.group_id import GroupId

        episodes = []

        try:
            # end_indexãŒæŒ‡å®šã•ã‚Œã¦ã„ãªã„å ´åˆã¯ã€å­˜åœ¨ã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¢ç´¢
            if end_index is None:
                end_index = start_index + 1000  # ä¸Šé™ã‚’è¨­å®šã—ã¦ç„¡é™ãƒ«ãƒ¼ãƒ—ã‚’é˜²ã

            for episode_index in range(start_index, end_index + 1):
                episode_file = self._get_episode_file_path(file_path, episode_index)

                if not episode_file.exists():
                    # end_indexãŒæŒ‡å®šã•ã‚Œã¦ã„ãªã„å ´åˆã¯ã€ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚‰ãªããªã£ãŸã‚‰çµ‚äº†
                    if end_index == start_index + 1000:
                        break
                    # end_indexãŒæŒ‡å®šã•ã‚Œã¦ã„ã‚‹å ´åˆã¯è­¦å‘Š
                    self._logger.warning(
                        f"âš ï¸ ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {episode_file}"
                    )
                    continue

                with open(episode_file, "r", encoding="utf-8") as f:
                    episode_data = json.load(f)

                # ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’å¾©å…ƒ
                episode = Episode(
                    name=episode_data["name"],
                    body=episode_data["body"],
                    source_description=episode_data["source_description"],
                    reference_time=datetime.fromisoformat(
                        episode_data["reference_time"]
                    )
                    if episode_data.get("reference_time")
                    else None,
                    episode_type=episode_data["episode_type"],
                    group_id=GroupId(episode_data["group_id"]),
                )
                episodes.append(episode)

            self._logger.info(
                f"ğŸ“ ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿å®Œäº†: {file_path} "
                f"({len(episodes)}ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰, ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹: {start_index}ã€œ{start_index + len(episodes) - 1})"
            )

            return episodes

        except (OSError, json.JSONDecodeError) as e:
            self._logger.error(f"âŒ ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿å¤±æ•—: {file_path} - {e}")
            raise

    def has_saved_episodes(self, file_path: str) -> bool:
        """
        æŒ‡å®šãƒ•ã‚¡ã‚¤ãƒ«ã®ä¿å­˜æ¸ˆã¿ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯ã™ã‚‹

        Args:
            file_path: å…ƒãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹

        Returns:
            bool: ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã™ã‚‹å ´åˆTrue
        """
        # ãƒãƒ£ãƒ³ã‚¯ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã«ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
        chunk_dir = self._get_chunk_directory(file_path)
        if not chunk_dir.exists():
            return False

        # episode_*.jsonãƒ•ã‚¡ã‚¤ãƒ«ã®å­˜åœ¨ã‚’ç¢ºèª
        episode_files = list(chunk_dir.glob("episode_*.json"))
        return len(episode_files) > 0

    def delete_episode_files(
        self,
        file_path: str,
        start_index: int = 0,
        end_index: Optional[int] = None,
    ) -> None:
        """
        æŒ‡å®šç¯„å›²ã®ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤ã™ã‚‹

        Args:
            file_path: å…ƒãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
            start_index: é–‹å§‹ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
            end_index: çµ‚äº†ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ï¼ˆNoneã®å ´åˆã¯å­˜åœ¨ã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ã¾ã§ï¼‰
        """
        deleted_count = 0

        try:
            # end_indexãŒæŒ‡å®šã•ã‚Œã¦ã„ãªã„å ´åˆã¯ã€å­˜åœ¨ã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¢ç´¢
            if end_index is None:
                end_index = start_index + 1000  # ä¸Šé™ã‚’è¨­å®š

            for episode_index in range(start_index, end_index + 1):
                episode_file = self._get_episode_file_path(file_path, episode_index)

                if episode_file.exists():
                    episode_file.unlink()
                    deleted_count += 1
                    self._logger.debug(f"ğŸ—‘ï¸ ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤: {episode_file}")
                elif end_index == start_index + 1000:
                    # æ¢ç´¢ãƒ¢ãƒ¼ãƒ‰ã§ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚‰ãªããªã£ãŸã‚‰çµ‚äº†
                    break

            if deleted_count > 0:
                self._logger.info(
                    f"ğŸ—‘ï¸ ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤å®Œäº†: {file_path} "
                    f"({deleted_count}ãƒ•ã‚¡ã‚¤ãƒ«, ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹: {start_index}ã€œ{start_index + deleted_count - 1})"
                )

                # å‰Šé™¤å¾Œã«ç©ºãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
                chunk_dir = self._get_chunk_directory(file_path)
                self._cleanup_empty_directories(chunk_dir)

        except OSError as e:
            self._logger.warning(f"âš ï¸ ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤å¤±æ•—: {file_path} - {e}")

    def _cleanup_empty_directories(self, directory: Path) -> None:
        """
        ç©ºã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’å†å¸°çš„ã«å‰Šé™¤ã™ã‚‹ï¼ˆchunks_directoryã¾ã§ã¯å‰Šé™¤ã—ãªã„ï¼‰

        Args:
            directory: å‰Šé™¤å¯¾è±¡ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        """
        if not directory.exists() or not directory.is_dir():
            return

        # chunks_directoryä»¥ä¸‹ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ã¿å‰Šé™¤å¯¾è±¡
        if not directory.is_relative_to(self._chunks_directory):
            return

        try:
            # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒç©ºã§ã€chunks_directoryã§ã¯ãªã„å ´åˆã«å‰Šé™¤
            if not any(directory.iterdir()) and directory != self._chunks_directory:
                directory.rmdir()
                self._logger.debug(f"ğŸ—‘ï¸ ç©ºãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå‰Šé™¤: {directory}")

                # è¦ªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚‚ç¢ºèªï¼ˆå†å¸°çš„ï¼‰
                self._cleanup_empty_directories(directory.parent)

        except OSError as e:
            self._logger.debug(f"ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå‰Šé™¤å¤±æ•—: {directory} - {e}")
