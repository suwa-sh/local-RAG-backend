# =====================================
# 環境変数設定例
# =====================================
# このファイルを .env にコピーして使用してください
# cp .env.example .env

# -------------------------------------
# ドキュメント登録、MCPサーバー 共通設定
# -------------------------------------
# Neo4j データベース設定（必須）
NEO4J_URI=bolt://localhost:7687

NEO4J_USER=neo4j

NEO4J_PASSWORD=password

# LLM API設定（必須）
# 推奨: OpenAI API直接接続（最高性能・最安定）
LLM_MODEL_URL=https://api.openai.com/v1

LLM_MODEL_KEY=your_openai_api_key_here

LLM_MODEL_NAME=gpt-4o-mini

# Rerankモデル（省略時はLLM_MODEL_NAMEと同じ）
RERANK_MODEL_NAME=gpt-4.1-nano

# 埋め込みモデル API設定（必須）
EMBEDDING_MODEL_URL=http://host.docker.internal:11434/v1

EMBEDDING_MODEL_KEY=dummy

EMBEDDING_MODEL_NAME=kun432/cl-nagoya-ruri-large:latest

# テナント識別子設定（必須）
GROUP_ID=default

# -------------------------------------
# ドキュメント登録設定
# -------------------------------------
# ログレベル設定
LOG_LEVEL=INFO

# チャンク分割設定
# チャンクの最大文字数（デフォルト: 2000）
CHUNK_SIZE_MAX=2000

# チャンクの最小文字数（デフォルト: 200）
CHUNK_SIZE_MIN=200

# チャンクのオーバーラップ（デフォルト: 0）
CHUNK_OVERLAP=0

# 並列処理設定
# チャンク処理の並列ワーカー数（デフォルト: 3）
INGEST_CHUNK_WORKERS=3

# 登録処理の並列ワーカー数（デフォルト: 2）
INGEST_REGISTER_WORKERS=2

# -------------------------------------
# MCPサーバー設定
# -------------------------------------
# LLM温度
# 0.0-1.0の範囲で設定。低い値ほど出力が決定的になる（デフォルト: 0.0）
# RAGシステムでは構造化出力のため、0.0-0.3を推奨
LLM_TEMPERATURE=0.0
